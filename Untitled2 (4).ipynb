{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing LPR_Net With MLC and Model Optimizations"
      ],
      "metadata": {
        "id": "yvocQv11sMNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's test the baseline model. We will be using CPU for all demonstrative purposes due to the lack of compute resources on GPU."
      ],
      "metadata": {
        "id": "4RWufBmwsXL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Test Dataset"
      ],
      "metadata": {
        "id": "O-Ew775tsSb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OKtB00a4m81F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('test_data'):\n",
        "    !unzip test_data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "FKcO3sLDsUGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxscript\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVO0F8XOnFcL",
        "outputId": "fca2dd32-fca1-4342-8e71-5f4d656a6c8c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Requirement already satisfied: onnxscript in /usr/local/lib/python3.10/dist-packages (0.1.0.dev20241208)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.17.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxscript) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.16->onnxscript) (4.25.5)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.20.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries Needed"
      ],
      "metadata": {
        "id": "PLDbrnh7sVVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imutils import paths\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import *\n",
        "import time\n",
        "from torch.autograd import Variable\n",
        "from types import SimpleNamespace\n",
        "from PIL import Image, ImageDraw, ImageFont"
      ],
      "metadata": {
        "id": "DYAe4epAndFC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Seeds"
      ],
      "metadata": {
        "id": "C-pntVg6sZ1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "metadata": {
        "id": "fJq318a4pe4a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Structure"
      ],
      "metadata": {
        "id": "V8rQhDd0sbUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to issues with importing the MaxPool3D layer to ONNX, I needed to manually squeeze and unsqueeze those layers."
      ],
      "metadata": {
        "id": "oziFh7oGwFQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class small_basic_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(small_basic_block, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out // 4, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out, kernel_size=1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class LPRNet(nn.Module):\n",
        "    def __init__(self, lpr_max_len, phase, class_num, dropout_rate):\n",
        "        super(LPRNet, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.class_num = class_num\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1), # 0\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),  # 2\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
        "            small_basic_block(ch_in=64, ch_out=128),    # *** 4 ***\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),  # 6\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
        "            small_basic_block(ch_in=64, ch_out=256),   # 8\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 10\n",
        "            small_basic_block(ch_in=256, ch_out=256),   # *** 11 ***\n",
        "            nn.BatchNorm2d(num_features=256),   # 12\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),  # 14\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),  # 16\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 18\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1), # 20\n",
        "            nn.BatchNorm2d(num_features=class_num),\n",
        "            nn.ReLU(),  # *** 22 ***\n",
        "        )\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=448+self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            # nn.BatchNorm2d(num_features=self.class_num),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Conv2d(in_channels=self.class_num, out_channels=self.lpr_max_len+1, kernel_size=3, stride=2),\n",
        "            # nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        keep_features = list()\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            # x = layer(x)\n",
        "            if i == 3 or i == 7 or i == 14:\n",
        "                x = x.unsqueeze(1)\n",
        "                x = layer(x)\n",
        "                x = x.squeeze(1)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "            if i in [2, 6, 13, 22]: # [2, 4, 8, 11, 22]\n",
        "                keep_features.append(x)\n",
        "\n",
        "        global_context = list()\n",
        "        for i, f in enumerate(keep_features):\n",
        "            if i in [0, 1]:\n",
        "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
        "            if i in [2]:\n",
        "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
        "            f_pow = torch.pow(f, 2)\n",
        "            f_mean = torch.mean(f_pow)\n",
        "            f = torch.div(f, f_mean)\n",
        "            global_context.append(f)\n",
        "\n",
        "        x = torch.cat(global_context, 1)\n",
        "        x = self.container(x)\n",
        "        logits = torch.mean(x, dim=2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def build_lprnet(lpr_max_len=8, phase=False, class_num=66, dropout_rate=0.5):\n",
        "\n",
        "    Net = LPRNet(lpr_max_len, phase, class_num, dropout_rate)\n",
        "\n",
        "    if phase == \"train\":\n",
        "        return Net.train()\n",
        "    else:\n",
        "        return Net.eval()"
      ],
      "metadata": {
        "id": "fSggeWdgni64"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "MsawcRJFssAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHARS = ['京', '沪', '津', '渝', '冀', '晋', '蒙', '辽', '吉', '黑',\n",
        "         '苏', '浙', '皖', '闽', '赣', '鲁', '豫', '鄂', '湘', '粤',\n",
        "         '桂', '琼', '川', '贵', '云', '藏', '陕', '甘', '青', '宁',\n",
        "         '新',\n",
        "         '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "         'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K',\n",
        "         'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
        "         'W', 'X', 'Y', 'Z', 'I', 'O', '-'\n",
        "         ]\n",
        "\n",
        "CHARS_DICT = {char:i for i, char in enumerate(CHARS)}\n",
        "\n",
        "class LPRDataLoader(Dataset):\n",
        "    def __init__(self, img_dir, imgSize, lpr_max_len, PreprocFun=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_paths = []\n",
        "        for i in range(len(img_dir)):\n",
        "            self.img_paths += [el for el in paths.list_images(img_dir[i])]\n",
        "        random.shuffle(self.img_paths)\n",
        "        self.img_size = imgSize\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        if PreprocFun is not None:\n",
        "            self.PreprocFun = PreprocFun\n",
        "        else:\n",
        "            self.PreprocFun = self.transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.img_paths[index]\n",
        "        Image = cv2.imread(filename)\n",
        "        height, width, _ = Image.shape\n",
        "        if height != self.img_size[1] or width != self.img_size[0]:\n",
        "            Image = cv2.resize(Image, self.img_size)\n",
        "        Image = self.PreprocFun(Image)\n",
        "\n",
        "        basename = os.path.basename(filename)\n",
        "        imgname, suffix = os.path.splitext(basename)\n",
        "        imgname = imgname.split(\"-\")[0].split(\"_\")[0]\n",
        "        label = list()\n",
        "        for c in imgname:\n",
        "            # one_hot_base = np.zeros(len(CHARS))\n",
        "            # one_hot_base[CHARS_DICT[c]] = 1\n",
        "            label.append(CHARS_DICT[c])\n",
        "\n",
        "        if len(label) == 8:\n",
        "            if self.check(label) == False:\n",
        "                print(imgname)\n",
        "                assert 0, \"Error label ^~^!!!\"\n",
        "\n",
        "        return Image, label, len(label)\n",
        "\n",
        "    def transform(self, img):\n",
        "        img = img.astype('float32')\n",
        "        img -= 127.5\n",
        "        img *= 0.0078125\n",
        "        img = np.transpose(img, (2, 0, 1))\n",
        "\n",
        "        return img\n",
        "\n",
        "    def check(self, label):\n",
        "        if label[2] != CHARS_DICT['D'] and label[2] != CHARS_DICT['F'] \\\n",
        "                and label[-1] != CHARS_DICT['D'] and label[-1] != CHARS_DICT['F']:\n",
        "            print(\"Error label, Please check!\")\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
        "\n",
        "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)"
      ],
      "metadata": {
        "id": "sWZatftfn07j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "\n",
        "    lprnet = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
        "    device = torch.device(\"cpu\")\n",
        "    lprnet.to(device)\n",
        "    print(\"Successful to build network!\")\n",
        "\n",
        "    # load pretrained model\n",
        "    if args.pretrained_model:\n",
        "        lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
        "        print(\"load pretrained model successful!\")\n",
        "    else:\n",
        "        print(\"[Error] Can't found pretrained mode, please check!\")\n",
        "        return False\n",
        "\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "    try:\n",
        "        Greedy_Decode_Eval(lprnet, test_dataset, args)\n",
        "    finally:\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "def Greedy_Decode_Eval(Net, datasets, args):\n",
        "    # TestNet = Net.eval()\n",
        "    epoch_size = len(datasets) // args.test_batch_size\n",
        "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
        "\n",
        "    Tp = 0\n",
        "    Tn_1 = 0\n",
        "    Tn_2 = 0\n",
        "    t1 = time.time()\n",
        "    for i in range(epoch_size):\n",
        "        # load train data\n",
        "        images, labels, lengths = next(batch_iterator)\n",
        "        start = 0\n",
        "        targets = []\n",
        "        for length in lengths:\n",
        "            label = labels[start:start+length]\n",
        "            targets.append(label)\n",
        "            start += length\n",
        "        targets = np.array([el.numpy() for el in targets])\n",
        "        imgs = images.numpy().copy()\n",
        "\n",
        "        if args.cuda:\n",
        "            images = Variable(images.cuda())\n",
        "        else:\n",
        "            images = Variable(images)\n",
        "\n",
        "        # forward\n",
        "        prebs = Net(images)\n",
        "        # greedy decode\n",
        "        prebs = prebs.cpu().detach().numpy()\n",
        "        preb_labels = list()\n",
        "        for i in range(prebs.shape[0]):\n",
        "            preb = prebs[i, :, :]\n",
        "            preb_label = list()\n",
        "            for j in range(preb.shape[1]):\n",
        "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "            no_repeat_blank_label = list()\n",
        "            pre_c = preb_label[0]\n",
        "            if pre_c != len(CHARS) - 1:\n",
        "                no_repeat_blank_label.append(pre_c)\n",
        "            for c in preb_label: # dropout repeate label and blank label\n",
        "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                    if c == len(CHARS) - 1:\n",
        "                        pre_c = c\n",
        "                    continue\n",
        "                no_repeat_blank_label.append(c)\n",
        "                pre_c = c\n",
        "            preb_labels.append(no_repeat_blank_label)\n",
        "        for i, label in enumerate(preb_labels):\n",
        "            # show image and its predict label\n",
        "            if args.show:\n",
        "                show(imgs[i], label, targets[i])\n",
        "            if len(label) != len(targets[i]):\n",
        "                Tn_1 += 1\n",
        "                continue\n",
        "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "                Tp += 1\n",
        "            else:\n",
        "                Tn_2 += 1\n",
        "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
        "    t2 = time.time()\n",
        "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n",
        "    return Acc\n",
        "\n",
        "def show(img, label, target):\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img *= 128.\n",
        "    img += 127.5\n",
        "    img = img.astype(np.uint8)\n",
        "\n",
        "    lb = \"\"\n",
        "    for i in label:\n",
        "        lb += CHARS[i]\n",
        "    tg = \"\"\n",
        "    for j in target.tolist():\n",
        "        tg += CHARS[int(j)]\n",
        "\n",
        "    flag = \"F\"\n",
        "    if lb == tg:\n",
        "        flag = \"T\"\n",
        "    img = cv2ImgAddText(img, lb, (0, 0))\n",
        "    cv2.imshow(\"test\", img)\n",
        "    print(\"target: \", tg, \" ### {} ### \".format(flag), \"predict: \", lb)\n",
        "    cv2.waitKey()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "def cv2ImgAddText(img, text, pos, textColor=(255, 0, 0), textSize=12):\n",
        "    if (isinstance(img, np.ndarray)):  # detect opencv format or not\n",
        "        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    fontText = ImageFont.truetype(\"data/NotoSansCJK-Regular.ttc\", textSize, encoding=\"utf-8\")\n",
        "    draw.text(pos, text, textColor, font=fontText)\n",
        "\n",
        "    return cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)"
      ],
      "metadata": {
        "id": "fBiy-QsMn8VA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Arguments"
      ],
      "metadata": {
        "id": "mdv7Hiz_tHR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = SimpleNamespace(**{\n",
        "    \"img_size\": [94, 24],\n",
        "    \"test_img_dirs\": \"./test_data\",\n",
        "    \"dropout_rate\": 0,\n",
        "    \"lpr_max_len\": 8,\n",
        "    \"test_batch_size\": 100,\n",
        "    \"phase_train\": False,\n",
        "    \"num_workers\": 0,\n",
        "    \"cuda\": False, # test on CPU\n",
        "    \"show\": False,\n",
        "    \"pretrained_model\": \"./Final_LPRNet_model.pth\",\n",
        "})"
      ],
      "metadata": {
        "id": "OaRpp7CopKpc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate the PyTorch baseline model's accuracy and inference speed per image."
      ],
      "metadata": {
        "id": "dcQ6WTUZiNjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOdFcfjfghby",
        "outputId": "0946eba7-1b02-4ae0-e086-fdaa5410e67f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "load pretrained model successful!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-b429f7fc8cf4>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.899 [899:59:42:1000]\n",
            "[Info] Test Speed: 0.21223733949661255s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build LPR Net Structure"
      ],
      "metadata": {
        "id": "wnKtgZ58xEzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size(model, filename):\n",
        "  dummy_input = torch.randn(1, 3, 24, 94)\n",
        "  torch.onnx.export(\n",
        "      lprnet,\n",
        "      dummy_input,\n",
        "      filename,\n",
        "      input_names=[\"input\"],\n",
        "      output_names=[\"output\"],\n",
        "      dynamic_axes={\n",
        "          \"input\": {0: \"batch_size\"},\n",
        "          \"output\": {0: \"batch_size\"},\n",
        "      },\n",
        "  )\n",
        "  size_in_bytes = os.path.getsize(filename)\n",
        "  size_in_mb = size_in_bytes / (1024 * 1024)\n",
        "  return size_in_mb\n",
        "\n",
        "def get_full_model_size_pytorch(model):\n",
        "    torch.save(model, \"full_model.pth\")\n",
        "    size = os.path.getsize(\"full_model.pth\")\n",
        "    os.remove('full_model.pth')\n",
        "    return size / 1e6  # Size in MB"
      ],
      "metadata": {
        "id": "nLK3Mrinnp6B"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lprnet = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
        "lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
        "print(\"Model Size: \" + str(get_model_size(lprnet, \"lprnet.onnx\")) + \" MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y-IFkt3oR0p",
        "outputId": "eb0b3bdf-e5a9-4763-a96a-82c4ab9c9514"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-42df1b960982>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 1.7072572708129883 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, we were able to obtain a test accuracy of 89.9%, an inference speed of 183 ms, and a model size of 1.71 MB."
      ],
      "metadata": {
        "id": "eWqwzlhFwnqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Size: \" + str(get_full_model_size_pytorch(lprnet)) + \" MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv3WMS4TUWbP",
        "outputId": "728bc45e-5444-47b6-c797-c8db3733eb88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: 1.82749 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also keep track of the PyTorch saving method, as it will be a useful comparison. And with this method the baseline model size is around 1.83 MB."
      ],
      "metadata": {
        "id": "k4UMUTzSUjqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning Optimizations"
      ],
      "metadata": {
        "id": "P1bzhotMi2xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's trying doing a filter-based pruning optimization."
      ],
      "metadata": {
        "id": "q2SPxDudjHAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model):\n",
        "  test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "  test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "  try:\n",
        "      Acc = Greedy_Decode_Eval(model, test_dataset, args)\n",
        "  finally:\n",
        "      cv2.destroyAllWindows()\n",
        "  return Acc"
      ],
      "metadata": {
        "id": "0dgs6Kd3njUA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "lprnet_copy = copy.deepcopy(lprnet)\n",
        "print(f\"LPRNet Model is on device: {next(lprnet_copy.parameters()).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND1H9NVjjmbH",
        "outputId": "ab8206f0-d83c-47b3-a3b1-38ad0d49536d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LPRNet Model is on device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def recover_model(model):\n",
        "    state_dict = torch.load(args.pretrained_model, map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "metadata": {
        "id": "e4mP_WQYkL7d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_prune(tensor: nn.Conv2d, sparsity: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Filter-based pruning for convolutional layers\n",
        "    :param tensor: torch.(cuda.)Tensor, weight of conv layer\n",
        "    :param sparsity: float, pruning sparsity\n",
        "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
        "    :return:\n",
        "        torch.(cuda.)Tensor, mask with zeroed indices\n",
        "    \"\"\"\n",
        "    num_filters = tensor.weight.size(0)\n",
        "    num_prune = int(num_filters * sparsity)\n",
        "\n",
        "    # L1-norm for each filter\n",
        "    flattened_tensor = tensor.weight.view(num_filters, -1)\n",
        "    filter_norms = torch.norm(flattened_tensor, p=1, dim=1)\n",
        "\n",
        "    # Prune filters with smallest L1-norm\n",
        "    prune_indices = torch.topk(filter_norms, num_prune, largest=False).indices\n",
        "\n",
        "    # Mask to determine what filters to keep / not keep\n",
        "    mask = torch.ones_like(filter_norms)\n",
        "    mask[prune_indices] = 0\n",
        "\n",
        "    # Apply mask to tensor weights\n",
        "    mask = mask.view(-1, 1, 1, 1)\n",
        "    tensor.weight.data.mul_(mask)\n",
        "\n",
        "    return mask\n",
        "\n",
        "class FilterPruner:\n",
        "    def __init__(self, model, sparsity_dict):\n",
        "        self.masks = FilterPruner.prune(model, sparsity_dict)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply(self, model):\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in self.masks:\n",
        "                param *= self.masks[name]\n",
        "\n",
        "    @staticmethod\n",
        "    @torch.no_grad()\n",
        "    def prune(model, sparsity_dict):\n",
        "        masks = dict()\n",
        "        for name, module in model.named_modules():\n",
        "            if isinstance(module, nn.Conv2d):\n",
        "                if isinstance(sparsity_dict, dict):\n",
        "                    masks[name] = filter_prune(module, sparsity_dict[name])\n",
        "                else:\n",
        "                    assert(0 <= sparsity_dict < 1)\n",
        "                    if sparsity_dict > 0:\n",
        "                        masks[name] = filter_prune(module, sparsity_dict)\n",
        "        return masks\n"
      ],
      "metadata": {
        "id": "MZh1YXPMjEr1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I orignally tried pruning up to 30% of the weight, but unfortunately it reduces the accuracy by a significant amount. As we have no efficient way of retraining the pruned model to gain back the lost accuracy, I decided to only prune the amount of weights that I could optimize, which ended up being only between 1% to 1.65%."
      ],
      "metadata": {
        "id": "i9hM6h5X0Uc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_accuracies_filter = []\n",
        "model_sizes_filter = []\n",
        "best_pruned_model = None\n",
        "\n",
        "for i in range(20, 34, 1): # (20, 34, 1)\n",
        "  sparsity = i / 2000\n",
        "  print(\"Sparsity factor: \", sparsity)\n",
        "  lprnet_copy = recover_model(lprnet_copy)\n",
        "  pruner = FilterPruner(lprnet_copy, sparsity)\n",
        "  pruner.apply(lprnet_copy)\n",
        "  sparse_model_accuracy = evaluate(lprnet_copy)\n",
        "  sparse_model_size = get_model_size(lprnet_copy, \"lprnet_model_prune_\" + str(sparsity) + \".onnx\")\n",
        "  model_accuracies_filter.append(sparse_model_accuracy)\n",
        "  model_sizes_filter.append(sparse_model_size)\n",
        "  print(\"Model Size: \" + str(sparse_model_size) + \" MB\")\n",
        "  # As soon as the pruned model performs worse than 85%, I break from the for loop.\n",
        "  if sparse_model_accuracy > 0.85:\n",
        "    best_pruned_model = copy.deepcopy(lprnet_copy)\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH8MI2TejLXP",
        "outputId": "9a772e5d-7982-444a-aacd-06284cf83da0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparsity factor:  0.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-6087602ff8ac>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.pretrained_model, map_location=torch.device('cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.9 [900:59:41:1000]\n",
            "[Info] Test Speed: 0.20528143215179442s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.0105\n",
            "[Info] Test Accuracy: 0.9 [900:61:39:1000]\n",
            "[Info] Test Speed: 0.20610330057144166s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.011\n",
            "[Info] Test Accuracy: 0.9 [900:59:41:1000]\n",
            "[Info] Test Speed: 0.20144831156730653s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.0115\n",
            "[Info] Test Accuracy: 0.897 [897:61:42:1000]\n",
            "[Info] Test Speed: 0.20327204990386963s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.012\n",
            "[Info] Test Accuracy: 0.903 [903:58:39:1000]\n",
            "[Info] Test Speed: 0.20221845483779907s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.0125\n",
            "[Info] Test Accuracy: 0.902 [902:59:39:1000]\n",
            "[Info] Test Speed: 0.2006907227039337s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.013\n",
            "[Info] Test Accuracy: 0.897 [897:61:42:1000]\n",
            "[Info] Test Speed: 0.20007882452011108s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.0135\n",
            "[Info] Test Accuracy: 0.898 [898:60:42:1000]\n",
            "[Info] Test Speed: 0.20203764009475708s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.014\n",
            "[Info] Test Accuracy: 0.901 [901:58:41:1000]\n",
            "[Info] Test Speed: 0.19993429374694824s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.0145\n",
            "[Info] Test Accuracy: 0.899 [899:62:39:1000]\n",
            "[Info] Test Speed: 0.1999263973236084s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n",
            "Sparsity factor:  0.015\n",
            "[Info] Test Accuracy: 0.341 [341:495:164:1000]\n",
            "[Info] Test Speed: 0.20081760501861573s 1/1000]\n",
            "Model Size: 1.7072572708129883 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the output above, the model size did not change significantly as pruning was taking place, which makes sense since I could only remove such a small amount of weights before the model performance was affected dramatically. I will choose the best model which can maximize the sparsity while not affecting the accuracy or speed as much. From the graph above, and the values outputted, I believe that a sparsity factor of 1.45% would be best as it generally keeps the size from 1.707 MB to 1.707 MB, accuracy from 90.1% to 89.9%, and inference time from 183.8 to 182.4. Although these changes are somewhat negligble, I was not able to do much better due to the lack of a provided dataset which would enable me to fine-tune the model after pruning the weights."
      ],
      "metadata": {
        "id": "ADHJ-ONG11F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of full model saved using PyTorch: \" + str(get_full_model_size_pytorch(best_pruned_model)) + \" MB\")"
      ],
      "metadata": {
        "id": "BHlHamJPT4y8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c625315-f1d3-4bc0-c524-6974e622a992"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of full model saved using PyTorch: 1.82749 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the size did not change, I decided to save the model using PyTorch, which keeps the weights and architecture, and I found that the best pruned model saved with a size of 1.82 MB. Comparing this to the baseline, we can also see that the model size did not change, suggesting that little improvement was done with pruning without dramatically affecting the performance."
      ],
      "metadata": {
        "id": "zIlXEn58T_Rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_pruned_model_copy = copy.deepcopy(best_pruned_model)\n",
        "best_pruned_model_copy"
      ],
      "metadata": {
        "id": "3UBfiuSSCg0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380c3dca-20d2-430a-c752-24ad76fbc6df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LPRNet(\n",
              "  (backbone): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU()\n",
              "        (2): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (3): ReLU()\n",
              "        (4): Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (6): ReLU()\n",
              "    (7): MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (8): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU()\n",
              "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (3): ReLU()\n",
              "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): ReLU()\n",
              "    (11): small_basic_block(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): ReLU()\n",
              "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
              "        (3): ReLU()\n",
              "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
              "        (5): ReLU()\n",
              "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "    )\n",
              "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (13): ReLU()\n",
              "    (14): MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "    (15): Dropout(p=0, inplace=False)\n",
              "    (16): Conv2d(64, 256, kernel_size=(1, 4), stride=(1, 1))\n",
              "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (18): ReLU()\n",
              "    (19): Dropout(p=0, inplace=False)\n",
              "    (20): Conv2d(256, 68, kernel_size=(13, 1), stride=(1, 1))\n",
              "    (21): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (22): ReLU()\n",
              "  )\n",
              "  (container): Sequential(\n",
              "    (0): Conv2d(516, 68, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantizations"
      ],
      "metadata": {
        "id": "9sFhnpgRt-Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After pruning, the next optimization that I chose to do was quantization. Specifically, I chose to apply post-training static quantization (PTQ) as I did not have a dataset to train the model after quantization. Nonetheless, we will apply this methodology to our model to see how it performs compared to pruning and the baseline itself."
      ],
      "metadata": {
        "id": "pPFRcOqACkqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(best_pruned_model_copy)"
      ],
      "metadata": {
        "id": "mAskWZIvnH32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f693c2bb-7b92-4af0-9879-c61e3b72e4bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.898 [898:61:41:1000]\n",
            "[Info] Test Speed: 0.19977479028701783s 1/1000]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.898"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "eVkzDcZVG0AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.quantization import QuantStub, DeQuantStub\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "QTGY0TT4IXXN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Test Dataset and Loader"
      ],
      "metadata": {
        "id": "h3syTP6yIYT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.num_workers, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "wcydOHvQe5k3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our specific methodology, we will need to wrap the MaxPool3D layers to ensure that those layers are contiguous."
      ],
      "metadata": {
        "id": "ayY06KISIk9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsureContiguous(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.contiguous()\n",
        "\n",
        "def insert_quant_stubs(module):\n",
        "    for name, submodule in module.named_children():\n",
        "        if isinstance(submodule, nn.MaxPool3d):\n",
        "            new_layer = nn.Sequential(\n",
        "                DeQuantStub(),\n",
        "                EnsureContiguous(),\n",
        "                submodule,\n",
        "                QuantStub()\n",
        "            )\n",
        "            setattr(module, name, new_layer)\n",
        "        elif isinstance(submodule, nn.Sequential):\n",
        "            insert_quant_stubs(submodule)\n",
        "    return module\n",
        "\n",
        "class LPRNetWithQuant(nn.Module):\n",
        "    \"\"\"\n",
        "    A wrapper class to add quantization support to the LPRNet model.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "        self.model = insert_quant_stubs(base_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x)\n",
        "\n",
        "        feature_maps = []\n",
        "        for idx, layer in enumerate(self.model.backbone.children()):\n",
        "            x = layer(x)\n",
        "            if idx in [2, 6, 13, 22]:\n",
        "                feature_maps.append(x)\n",
        "\n",
        "        context_features = []\n",
        "        for idx, fmap in enumerate(feature_maps):\n",
        "            if idx in [0, 1]:\n",
        "                fmap = nn.AvgPool2d(kernel_size=5, stride=5)(fmap)\n",
        "            elif idx == 2:\n",
        "                fmap = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(fmap)\n",
        "\n",
        "            fmap = self.dequant(fmap)\n",
        "            normalized_fmap = fmap / torch.mean(torch.pow(fmap, 2))\n",
        "            fmap = self.quant(normalized_fmap)\n",
        "            context_features.append(fmap)\n",
        "\n",
        "        x = torch.cat(context_features, dim=1)\n",
        "        x = self.model.container(x)\n",
        "        logits = torch.mean(x, dim=2)\n",
        "        return self.dequant(logits).contiguous()"
      ],
      "metadata": {
        "id": "kukRO38tXiOK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_model_for_quantization(base_model):\n",
        "    \"\"\"\n",
        "    Prepares the model for post-training quantization.\n",
        "    \"\"\"\n",
        "    quant_ready_model = LPRNetWithQuant(base_model)\n",
        "    quant_ready_model.eval()\n",
        "    return quant_ready_model\n",
        "\n",
        "def perform_calibration(model, data_loader, max_batches=100):\n",
        "    device = next(model.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, _, _) in enumerate(data_loader):\n",
        "            if batch_idx >= max_batches:\n",
        "                break\n",
        "            print(f\"Calibrating batch {batch_idx + 1}/{max_batches}\")\n",
        "            images = images.to(device)\n",
        "            model(images)\n",
        "\n",
        "# Configure quantization\n",
        "torch.backends.quantized.engine = 'fbgemm' # x86 architecture\n",
        "best_pruned_model_copy = prepare_model_for_quantization(best_pruned_model_copy)\n",
        "best_pruned_model_copy.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
        "\n",
        "# Prepare and calibrate\n",
        "torch.quantization.prepare(best_pruned_model_copy, inplace=True)\n",
        "perform_calibration(best_pruned_model_copy, test_loader, max_batches=10)\n",
        "\n",
        "# Convert to quantized model\n",
        "torch.quantization.convert(best_pruned_model_copy, inplace=True)\n",
        "\n",
        "print(\"Quantization complete!\")"
      ],
      "metadata": {
        "id": "iTk_x1-PKa0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54c738e-de22-4d7d-b179-c797f2ae1892"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrating batch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrating batch 2/10\n",
            "Calibrating batch 3/10\n",
            "Calibrating batch 4/10\n",
            "Calibrating batch 5/10\n",
            "Calibrating batch 6/10\n",
            "Calibrating batch 7/10\n",
            "Calibrating batch 8/10\n",
            "Calibrating batch 9/10\n",
            "Calibrating batch 10/10\n",
            "Quantization complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_quantized_model = copy.deepcopy(best_pruned_model_copy)\n",
        "evaluate(pruned_quantized_model)\n",
        "print(\"Size of pruned and quantized model: \" + str(get_model_size(pruned_quantized_model, \"lprnet_model_prune_\" + str(sparsity) + \"_quantized.onnx\")) + \" MB\")"
      ],
      "metadata": {
        "id": "wDE-hu76CBSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcc141a-bcfa-47ce-dc4f-f80f4f8ea043"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.723 [723:164:113:1000]\n",
            "[Info] Test Speed: 0.026481295347213746s 1/1000]\n",
            "Size of pruned and quantized model: 1.7072572708129883 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test accuracy reduced fairly significantly from 90% to 71.4% with quantization, but the inference speed also improved dramatically from 188.4 ms to 28.8 ms. Unfortunately, the model size does not change when I save it to an ONNX format, so let's trying saving it using PyTorch"
      ],
      "metadata": {
        "id": "vtMDvWmWD6Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of full model saved using PyTorch: \" + str(get_full_model_size_pytorch(pruned_quantized_model)) + \" MB\")"
      ],
      "metadata": {
        "id": "D1zJpApXDxo_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ec4b3ba-1499-4e1f-fee1-7d806b9218e3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of full model saved using PyTorch: 0.536304 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From saving using PyTorch, which captures the weights and architecture but in a different format, I got a model size of 0.54 MB, which is significantly smaller than the 1.83 MB from pruning."
      ],
      "metadata": {
        "id": "4wYRKg2dTfdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pruned_quantized_model"
      ],
      "metadata": {
        "id": "pvLFSyvqVieV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d81b477-7b15-4f7d-9002-18d208dcd28c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LPRNetWithQuant(\n",
              "  (quant): Quantize(scale=tensor([0.3139]), zero_point=tensor([3]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (model): LPRNet(\n",
              "    (backbone): Sequential(\n",
              "      (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.3160325884819031, zero_point=56)\n",
              "      (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Sequential(\n",
              "        (0): DeQuantize()\n",
              "        (1): EnsureContiguous()\n",
              "        (2): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
              "        (3): Quantize(scale=tensor([0.1786]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      )\n",
              "      (4): small_basic_block(\n",
              "        (block): Sequential(\n",
              "          (0): QuantizedConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.42272183299064636, zero_point=71)\n",
              "          (1): ReLU()\n",
              "          (2): QuantizedConv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), scale=0.8551036715507507, zero_point=69, padding=(1, 0))\n",
              "          (3): ReLU()\n",
              "          (4): QuantizedConv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), scale=2.730538845062256, zero_point=42, padding=(0, 1))\n",
              "          (5): ReLU()\n",
              "          (6): QuantizedConv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), scale=3.6506521701812744, zero_point=65)\n",
              "        )\n",
              "      )\n",
              "      (5): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (6): ReLU()\n",
              "      (7): Sequential(\n",
              "        (0): DeQuantize()\n",
              "        (1): EnsureContiguous()\n",
              "        (2): MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "        (3): Quantize(scale=tensor([0.4650]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      )\n",
              "      (8): small_basic_block(\n",
              "        (block): Sequential(\n",
              "          (0): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.8494337201118469, zero_point=60)\n",
              "          (1): ReLU()\n",
              "          (2): QuantizedConv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), scale=1.4190027713775635, zero_point=50, padding=(1, 0))\n",
              "          (3): ReLU()\n",
              "          (4): QuantizedConv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), scale=2.2697460651397705, zero_point=58, padding=(0, 1))\n",
              "          (5): ReLU()\n",
              "          (6): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=2.0976922512054443, zero_point=63)\n",
              "        )\n",
              "      )\n",
              "      (9): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (10): ReLU()\n",
              "      (11): small_basic_block(\n",
              "        (block): Sequential(\n",
              "          (0): QuantizedConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.25665053725242615, zero_point=41)\n",
              "          (1): ReLU()\n",
              "          (2): QuantizedConv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), scale=0.35725846886634827, zero_point=68, padding=(1, 0))\n",
              "          (3): ReLU()\n",
              "          (4): QuantizedConv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), scale=0.4292910397052765, zero_point=53, padding=(0, 1))\n",
              "          (5): ReLU()\n",
              "          (6): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.9712840914726257, zero_point=33)\n",
              "        )\n",
              "      )\n",
              "      (12): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (13): ReLU()\n",
              "      (14): Sequential(\n",
              "        (0): DeQuantize()\n",
              "        (1): EnsureContiguous()\n",
              "        (2): MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
              "        (3): Quantize(scale=tensor([0.0494]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "      )\n",
              "      (15): QuantizedDropout(p=0, inplace=False)\n",
              "      (16): QuantizedConv2d(64, 256, kernel_size=(1, 4), stride=(1, 1), scale=0.14894923567771912, zero_point=75)\n",
              "      (17): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (18): ReLU()\n",
              "      (19): QuantizedDropout(p=0, inplace=False)\n",
              "      (20): QuantizedConv2d(256, 68, kernel_size=(13, 1), stride=(1, 1), scale=0.21436432003974915, zero_point=68)\n",
              "      (21): QuantizedBatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (22): ReLU()\n",
              "    )\n",
              "    (container): Sequential(\n",
              "      (0): QuantizedConv2d(516, 68, kernel_size=(1, 1), stride=(1, 1), scale=1.7822608947753906, zero_point=92)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TVM Optimizations"
      ],
      "metadata": {
        "id": "eRmRljJayXjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Apache TVM"
      ],
      "metadata": {
        "id": "UQJgOagI3aR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-tvm"
      ],
      "metadata": {
        "id": "qL8u14Z61enu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e42e47-7518-4569-c4f5-9343ffbfdf6b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: apache-tvm in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (3.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.26.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.13.1)\n",
            "Requirement already satisfied: synr==0.6.0 in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (0.6.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "from tvm import relay\n",
        "import onnx\n",
        "from tvm.contrib import graph_executor\n",
        "\n",
        "# Load the ONNX model and Convert to TVM\n",
        "onnx_model = onnx.load(\"lprnet_model_prune_0.0145_quantized.onnx\")\n",
        "\n",
        "mod, params = relay.frontend.from_onnx(onnx_model, shape={\"input\": (100, 3, 24, 94)})\n",
        "\n",
        "print(mod)"
      ],
      "metadata": {
        "id": "7ZF5eHqZ3ZmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3c7dc34-dde8-4660-f59d-00f60b6e9b54"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def @main(%input: Tensor[(100, 3, 24, 94), float32] /* ty=Tensor[(100, 3, 24, 94), float32] */) -> Tensor[(100, 68, 18), float32] {\n",
            "  %0 = nn.conv2d(%input, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
            "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
            "  %2 = nn.relu(%1) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
            "  %3 = nn.avg_pool2d(%2, pool_size=[5, 5], strides=[5, 5], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
            "  %4 = power(%3, 2f /* ty=float32 */) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
            "  %5 = mean(%4, axis=[0, 1, 2, 3]) /* ty=float32 */;\n",
            "  %6 = expand_dims(%2, axis=1) /* ty=Tensor[(100, 1, 64, 22, 92), float32] */;\n",
            "  %7 = nn.max_pool3d(%6, pool_size=[1, 3, 3], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 20, 90), float32] */;\n",
            "  %8 = squeeze(%7, axis=[1]) /* ty=Tensor[(100, 64, 20, 90), float32] */;\n",
            "  %9 = nn.conv2d(%8, meta[relay.Constant][2] /* ty=Tensor[(32, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %10 = nn.bias_add(%9, meta[relay.Constant][3] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %11 = nn.relu(%10) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %12 = nn.conv2d(%11, meta[relay.Constant][4] /* ty=Tensor[(32, 32, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=32, kernel_size=[3, 1]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %13 = nn.bias_add(%12, meta[relay.Constant][5] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %14 = nn.relu(%13) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %15 = nn.conv2d(%14, meta[relay.Constant][6] /* ty=Tensor[(32, 32, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=32, kernel_size=[1, 3]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %16 = nn.bias_add(%15, meta[relay.Constant][7] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %17 = nn.relu(%16) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %18 = nn.conv2d(%17, meta[relay.Constant][8] /* ty=Tensor[(128, 32, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
            "  %19 = nn.bias_add(%18, meta[relay.Constant][9] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
            "  %20 = nn.relu(%19) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
            "  %21 = nn.avg_pool2d(%20, pool_size=[5, 5], strides=[5, 5], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
            "  %22 = power(%21, 2f /* ty=float32 */) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
            "  %23 = mean(%22, axis=[0, 1, 2, 3]) /* ty=float32 */;\n",
            "  %24 = expand_dims(%20, axis=1) /* ty=Tensor[(100, 1, 128, 20, 90), float32] */;\n",
            "  %25 = nn.max_pool3d(%24, pool_size=[1, 3, 3], strides=[2, 1, 2], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 18, 44), float32] */;\n",
            "  %26 = squeeze(%25, axis=[1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %27 = nn.conv2d(%26, meta[relay.Constant][10] /* ty=Tensor[(64, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %28 = nn.bias_add(%27, meta[relay.Constant][11] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %29 = nn.relu(%28) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %30 = nn.conv2d(%29, meta[relay.Constant][12] /* ty=Tensor[(64, 64, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=64, kernel_size=[3, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %31 = nn.bias_add(%30, meta[relay.Constant][13] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %32 = nn.relu(%31) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %33 = nn.conv2d(%32, meta[relay.Constant][14] /* ty=Tensor[(64, 64, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=64, kernel_size=[1, 3]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %34 = nn.bias_add(%33, meta[relay.Constant][15] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %35 = nn.relu(%34) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %36 = nn.conv2d(%35, meta[relay.Constant][16] /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %37 = nn.bias_add(%36, meta[relay.Constant][17] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %38 = nn.relu(%37) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %39 = nn.conv2d(%38, meta[relay.Constant][18] /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %40 = nn.bias_add(%39, meta[relay.Constant][19] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %41 = nn.relu(%40) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %42 = nn.conv2d(%41, meta[relay.Constant][20] /* ty=Tensor[(64, 64, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=64, kernel_size=[3, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %43 = nn.bias_add(%42, meta[relay.Constant][21] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %44 = nn.relu(%43) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %45 = nn.conv2d(%44, meta[relay.Constant][22] /* ty=Tensor[(64, 64, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=64, kernel_size=[1, 3]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %46 = nn.bias_add(%45, meta[relay.Constant][23] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %47 = nn.relu(%46) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %48 = nn.conv2d(%47, meta[relay.Constant][24] /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %49 = nn.bias_add(%48, meta[relay.Constant][25] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %50 = nn.relu(%49) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %51 = nn.avg_pool2d(%50, pool_size=[4, 10], strides=[4, 2], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
            "  %52 = power(%51, 2f /* ty=float32 */) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
            "  %53 = mean(%52, axis=[0, 1, 2, 3]) /* ty=float32 */;\n",
            "  %54 = expand_dims(%50, axis=1) /* ty=Tensor[(100, 1, 256, 18, 44), float32] */;\n",
            "  %55 = nn.max_pool3d(%54, pool_size=[1, 3, 3], strides=[4, 1, 2], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 16, 21), float32] */;\n",
            "  %56 = squeeze(%55, axis=[1]) /* ty=Tensor[(100, 64, 16, 21), float32] */;\n",
            "  %57 = nn.conv2d(%56, meta[relay.Constant][26] /* ty=Tensor[(256, 64, 1, 4), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 4]) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
            "  %58 = nn.bias_add(%57, meta[relay.Constant][27] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
            "  %59 = nn.relu(%58) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
            "  %60 = nn.conv2d(%59, meta[relay.Constant][28] /* ty=Tensor[(68, 256, 13, 1), float32] */, padding=[0, 0, 0, 0], channels=68, kernel_size=[13, 1]) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %61 = nn.bias_add(%60, meta[relay.Constant][29] /* ty=Tensor[(68), float32] */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %62 = nn.relu(%61) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %63 = power(%62, 2f /* ty=float32 */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %64 = mean(%63, axis=[0, 1, 2, 3]) /* ty=float32 */;\n",
            "  %65 = divide(%3, %5) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
            "  %66 = divide(%21, %23) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
            "  %67 = divide(%51, %53) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
            "  %68 = divide(%62, %64) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %69 = (%65, %66, %67, %68) /* ty=(Tensor[(100, 64, 4, 18), float32], Tensor[(100, 128, 4, 18), float32], Tensor[(100, 256, 4, 18), float32], Tensor[(100, 68, 4, 18), float32]) */;\n",
            "  %70 = concatenate(%69, axis=1) /* ty=Tensor[(100, 516, 4, 18), float32] */;\n",
            "  %71 = nn.conv2d(%70, meta[relay.Constant][30] /* ty=Tensor[(68, 516, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=68, kernel_size=[1, 1]) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %72 = nn.bias_add(%71, meta[relay.Constant][31] /* ty=Tensor[(68), float32] */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  mean(%72, axis=[2]) /* ty=Tensor[(100, 68, 18), float32] */\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
        "dev = tvm.cpu(0)"
      ],
      "metadata": {
        "id": "M08bch-L0Z_k"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_module(mod):\n",
        "    with tvm.transform.PassContext(opt_level=3):\n",
        "        lib = relay.build(mod, target=target, params=params)\n",
        "\n",
        "    dtype = \"float32\"\n",
        "    module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "\n",
        "    return module"
      ],
      "metadata": {
        "id": "wIHz48bfYLVd"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Greedy_Decode_Eval_TVM(module, datasets, args):\n",
        "    epoch_size = len(datasets) // args.test_batch_size\n",
        "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
        "\n",
        "    Tp = 0\n",
        "    Tn_1 = 0\n",
        "    Tn_2 = 0\n",
        "    t1 = time.time()\n",
        "\n",
        "    for i in range(epoch_size):\n",
        "        # Load batch data\n",
        "        images, labels, lengths = next(batch_iterator)\n",
        "        start = 0\n",
        "        targets = []\n",
        "        for length in lengths:\n",
        "            label = labels[start:start + length]\n",
        "            targets.append(label)\n",
        "            start += length\n",
        "        targets = np.array([el.numpy() for el in targets])\n",
        "        imgs = images.numpy().copy()\n",
        "\n",
        "        # Set TVM inputs and run inference\n",
        "        module.set_input(\"input\", tvm.nd.array(images.numpy()))\n",
        "        module.run()\n",
        "        prebs = module.get_output(0).asnumpy()\n",
        "\n",
        "        # Greedy decode\n",
        "        preb_labels = list()\n",
        "        for i in range(prebs.shape[0]):\n",
        "            preb = prebs[i, :, :]\n",
        "            preb_label = list()\n",
        "            for j in range(preb.shape[1]):\n",
        "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "            no_repeat_blank_label = list()\n",
        "            pre_c = preb_label[0]\n",
        "            if pre_c != len(CHARS) - 1:\n",
        "                no_repeat_blank_label.append(pre_c)\n",
        "            for c in preb_label:\n",
        "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                    if c == len(CHARS) - 1:\n",
        "                        pre_c = c\n",
        "                    continue\n",
        "                no_repeat_blank_label.append(c)\n",
        "                pre_c = c\n",
        "            preb_labels.append(no_repeat_blank_label)\n",
        "\n",
        "        # Evaluate accuracy\n",
        "        for i, label in enumerate(preb_labels):\n",
        "            if args.show:\n",
        "                show(imgs[i], label, targets[i])\n",
        "            if len(label) != len(targets[i]):\n",
        "                Tn_1 += 1\n",
        "                continue\n",
        "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "                Tp += 1\n",
        "            else:\n",
        "                Tn_2 += 1\n",
        "\n",
        "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp + Tn_1 + Tn_2)))\n",
        "    t2 = time.time()\n",
        "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))"
      ],
      "metadata": {
        "id": "H_BPKr3JYL8n"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_base = build_module(mod)\n",
        "Greedy_Decode_Eval_TVM(mod_base, test_dataset, args)"
      ],
      "metadata": {
        "id": "xD0I5SCfYbcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86843c43-9687-41ea-8d35-935678aaccb4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autotvm:One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.897 [897:62:41:1000]\n",
            "[Info] Test Speed: 0.043021236419677734s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, it seems like the accuracy loss has recovered after importing to TVM, going from 71.7% to 89.9%. Moreover, the speed has also remained consistent although slightly slower, from 28.9 ms to 45.8 ms."
      ],
      "metadata": {
        "id": "8f6sYj-sbehE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specific TVM Optimizations"
      ],
      "metadata": {
        "id": "RmPARYGdd68e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TVM Quantization"
      ],
      "metadata": {
        "id": "a6RiYv6p1bMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_copy = copy.deepcopy(mod)"
      ],
      "metadata": {
        "id": "YXBGvXQX_U7y"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm import relay\n",
        "from tvm.contrib import graph_executor\n",
        "\n",
        "with relay.quantize.qconfig(\n",
        "    calibrate_mode=\"kl_divergence\",\n",
        "    global_scale=8.0,\n",
        "):\n",
        "    mod_optimized = relay.quantize.quantize(mod_copy, params)"
      ],
      "metadata": {
        "id": "tEwlwgIKjubU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d71eae85-e19d-42bd-f55c-ae8212428813"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Traceback (most recent call last):\n  7: TVMFuncCall\n  6: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::transform::Pass, tvm::IRModule)>::AssignTypedLambda<tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}>(tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}, std::string)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  5: tvm::transform::Pass::operator()(tvm::IRModule) const\n  4: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  3: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  2: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  1: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/usr/local/lib/python3.10/dist-packages/tvm/relay/quantize/_calibrate.py\", line 221, in wrapped_func\n    input_scale_func = _kl_scale(mod, dataset)\n  File \"/usr/local/lib/python3.10/dist-packages/tvm/relay/quantize/_calibrate.py\", line 97, in _kl_scale\n    for samples in collect_stats(mod, dataset, chunk_by):\n  File \"/usr/local/lib/python3.10/dist-packages/tvm/relay/quantize/_calibrate.py\", line 85, in collect_stats\n    for batch in dataset:\nTypeError: 'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d3492a78f4c2>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mglobal_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m ):\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmod_optimized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tvm/relay/quantize/quantize.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(mod, params, dataset)\u001b[0m\n\u001b[1;32m    368\u001b[0m     ):\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mquantize_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0mmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantize_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0mq_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_qconfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tvm/ir/transform.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, mod)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ffi_transform_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunPass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.PackedFuncBase.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./base.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.CHECK_CALL\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Traceback (most recent call last):\n  7: TVMFuncCall\n  6: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::transform::Pass, tvm::IRModule)>::AssignTypedLambda<tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}>(tvm::transform::{lambda(tvm::transform::Pass, tvm::IRModule)#7}, std::string)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  5: tvm::transform::Pass::operator()(tvm::IRModule) const\n  4: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  3: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  2: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  1: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  0: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<TVMFuncCreateFromCFunc::{lambda(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)#2}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) [clone .cold]\n  File \"tvm/_ffi/_cython/./packed_func.pxi\", line 56, in tvm._ffi._cy3.core.tvm_callback\n  File \"/usr/local/lib/python3.10/dist-packages/tvm/relay/quantize/_calibrate.py\", line 221, in wrapped_func\n    input_scale_func = _kl_scale(mod, dataset)\n  File \"/usr/local/lib/python3.10/dist-packages/tvm/relay/quantize/_calibrate.py\", line 97, in _kl_scale\n    for samples in collect_stats(mod, dataset, chunk_by):\n  File \"/usr/local/lib/python3.10/dist-packages/tvm/relay/quantize/_calibrate.py\", line 85, in collect_stats\n    for batch in dataset:\nTypeError: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computation Graph Optimization Pipeline\n",
        "\n",
        "* Graph Simplification: SimplifyInference, FoldConstant, FoldScaleAxis\n",
        "* Computation Optimization: FuseOps, EliminateCommonSubexpr\n",
        "* Graph Pruning: DeadCodeElimination\n",
        "* Layout Optimization: AlterOpLayout, ConvertLayout\n",
        "\n"
      ],
      "metadata": {
        "id": "GZcICH9wjkfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm.relay import transform\n",
        "\n",
        "# Define optimization passes\n",
        "passes = [\n",
        "    transform.SimplifyInference(),  # Simplify inference computations (e.g., BatchNorm folding)\n",
        "    transform.FuseOps(fuse_opt_level=2),  # Fuse Conv2D, ReLU, and BiasAdd into a single kernel\n",
        "    transform.EliminateCommonSubexpr(),  # Remove duplicate computations\n",
        "    transform.DeadCodeElimination(),  # Remove unused outputs and operations\n",
        "    transform.AlterOpLayout(),  # Transform operations for better performance on the target hardware\n",
        "    transform.FoldConstant(),  # Fold constant computations\n",
        "    transform.FoldScaleAxis(),  # Fold scaling factors for performance gains\n",
        "    transform.ConvertLayout({\"conv2d\": [\"NCHW\", \"NHWC\"]}),\n",
        "]\n",
        "\n",
        "# Apply the optimization pipeline\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    mod_optimized = tvm.transform.Sequential(passes)(mod_copy)"
      ],
      "metadata": {
        "id": "HZTkBhUyd5eu"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mod_optimized_copy = copy.deepcopy(mod_optimized)\n",
        "mod_optimized_copy = build_module(mod_optimized_copy)\n",
        "Greedy_Decode_Eval_TVM(mod_optimized_copy, test_dataset, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iitV8c_Y1Evl",
        "outputId": "8b70b190-24b3-4d81-a64d-5167c5ebb5aa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.899 [899:60:41:1000]\n",
            "[Info] Test Speed: 0.04566181373596191s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoTVM"
      ],
      "metadata": {
        "id": "8he-ittOcw8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_optimized_autotvm = copy.deepcopy(mod_optimized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "PwyytpcMAAp9",
        "outputId": "47c407e6-ee28-4613-f350-1b5e2bd7c3d6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mod_optimized' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-9d7d781439ad>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmod_optimized_autotvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_optimized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'mod_optimized' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm import auto_scheduler, relay, rpc\n",
        "from tvm.contrib import utils, ndk\n",
        "from tvm.relay import transform\n",
        "\n",
        "# Define the tuning task\n",
        "tasks, task_weights = auto_scheduler.extract_tasks(\n",
        "    mod_optimized_autotvm[\"main\"], params, target\n",
        ")\n",
        "\n",
        "# Define the log file to save the tuning records\n",
        "log_file = \"tuning_records.json\"\n",
        "\n",
        "# Set up the TaskScheduler\n",
        "tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
        "\n",
        "# Set up the runner and builder (adjust according to your environment)\n",
        "builder = auto_scheduler.LocalBuilder()\n",
        "runner = auto_scheduler.LocalRunner(repeat=10, min_repeat_ms=100, enable_cpu_cache_flush=True)\n",
        "\n",
        "# Run the tuning process\n",
        "tuner.tune(\n",
        "    auto_scheduler.TuningOptions(\n",
        "        num_measure_trials=500,  # Number of trials\n",
        "        builder=builder,\n",
        "        runner=runner,\n",
        "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "5h091FA3i5Le",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "fb6451e1-eac8-4878-ff23-5e4042af4379"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mod_optimized_autotvm' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0d76b564f81a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define the tuning task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m tasks, task_weights = auto_scheduler.extract_tasks(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmod_optimized_autotvm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"main\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mod_optimized_autotvm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm import auto_scheduler\n",
        "\n",
        "with auto_scheduler.ApplyHistoryBest(\"tuning_records.json\"):\n",
        "    with tvm.transform.PassContext(opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}):\n",
        "        lib_optimized_autotvm = relay.build(mod_optimized_autotvm, target=target, params=params)"
      ],
      "metadata": {
        "id": "eSHbvNBvb37L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating TVM Optimized Script"
      ],
      "metadata": {
        "id": "60DtV-C7cMYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm.contrib import graph_executor\n",
        "\n",
        "dtype = \"float32\"\n",
        "module_optimized_autotvm_graph = graph_executor.GraphModule(lib_optimized_autotvm[\"default\"](dev))"
      ],
      "metadata": {
        "id": "vn7yuv3ecLnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Greedy_Decode_Eval_TVM(module_optimized_autotvm_graph, test_dataset, args)"
      ],
      "metadata": {
        "id": "hUqnk8bscZzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ij72hx5wazgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with tvm.transform.PassContext(opt_level=3):  # Level 3 is for heavy optimizations\n",
        "#     mod = relay.transform.FoldConstant()(mod)  # Fold constants into the graph (e.g., for batchnorm, conv, etc.)\n",
        "#     # mod = relay.transform.MergeComposite()(mod)  # This will try to merge compatible operators into one\n",
        "#     mod = relay.transform.FuseOps()(mod)  # Fuse operations like Conv2d + ReLU into a single kernel\n",
        "\n",
        "# # Step 4: Target the compilation (e.g., CPU or CUDA)\n",
        "# target = \"llvm\"  # Use \"cuda\" for GPU, \"llvm\" for CPU\n",
        "# dev = tvm.device(target, 0)\n",
        "\n",
        "# # Step 5: Compile the model with the applied optimizations\n",
        "# with tvm.transform.PassContext(opt_level=3):\n",
        "#     # Apply the optimizations and build the model for the target\n",
        "#     compiled_lib = relay.build(mod, target, params=params)\n",
        "\n",
        "# # Step 6: Check the compiled model (optional)\n",
        "# # You can print out the transformed Relay module or inspect the optimized operators\n",
        "# print(mod)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MeGV6sP8Ioi",
        "outputId": "fd85be07-fc92-42c3-8545-09e615744b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def @main(%input: Tensor[(100, 3, 24, 94), float32] /* ty=Tensor[(100, 3, 24, 94), float32] */) -> Tensor[(100, 68, 18), float32] {\n",
            "  %8 = fn (%p04: Tensor[(100, 3, 24, 94), float32] /* ty=Tensor[(100, 3, 24, 94), float32] */, %p12: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %p22: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 22, 92), float32] {\n",
            "    %6 = nn.conv2d(%p04, %p12, padding=[0, 0, 0, 0], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
            "    %7 = nn.bias_add(%6, %p22) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
            "    nn.relu(%7) /* ty=Tensor[(100, 64, 22, 92), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 3, 24, 94), float32], Tensor[(64, 3, 3, 3), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 22, 92), float32] */;\n",
            "  %9 = %8(%input, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 3, 3), float32] */, meta[relay.Constant][1] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
            "  %10 = fn (%p03: Tensor[(100, 64, 22, 92), float32] /* ty=Tensor[(100, 64, 22, 92), float32] */, Primitive=1) -> Tensor[(100, 64, 4, 18), float32] {\n",
            "    nn.avg_pool2d(%p03, pool_size=[5, 5], strides=[5, 5], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 64, 4, 18), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 22, 92), float32]) -> Tensor[(100, 64, 4, 18), float32] */;\n",
            "  %12 = %10(%9) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
            "  %13 = fn (%p05: Tensor[(100, 64, 4, 18), float32] /* ty=Tensor[(100, 64, 4, 18), float32] */, Primitive=1) -> float32 {\n",
            "    %11 = power(%p05, 2f /* ty=float32 */) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
            "    mean(%11, axis=[0, 1, 2, 3]) /* ty=float32 */\n",
            "  } /* ty=fn (Tensor[(100, 64, 4, 18), float32]) -> float32 */;\n",
            "  %22 = fn (%p013: Tensor[(100, 64, 22, 92), float32] /* ty=Tensor[(100, 64, 22, 92), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 1, 64, 22, 92), float32] {\n",
            "    expand_dims(%p013, axis=1) /* ty=Tensor[(100, 1, 64, 22, 92), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 22, 92), float32]) -> Tensor[(100, 1, 64, 22, 92), float32] */;\n",
            "  %23 = %22(%9) /* ty=Tensor[(100, 1, 64, 22, 92), float32] */;\n",
            "  %24 = fn (%p012: Tensor[(100, 1, 64, 22, 92), float32] /* ty=Tensor[(100, 1, 64, 22, 92), float32] */, Primitive=1) -> Tensor[(100, 1, 64, 20, 90), float32] {\n",
            "    nn.max_pool3d(%p012, pool_size=[1, 3, 3], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 20, 90), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 1, 64, 22, 92), float32]) -> Tensor[(100, 1, 64, 20, 90), float32] */;\n",
            "  %25 = %24(%23) /* ty=Tensor[(100, 1, 64, 20, 90), float32] */;\n",
            "  %26 = fn (%p011: Tensor[(100, 1, 64, 20, 90), float32] /* ty=Tensor[(100, 1, 64, 20, 90), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 64, 20, 90), float32] {\n",
            "    squeeze(%p011, axis=[1]) /* ty=Tensor[(100, 64, 20, 90), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 1, 64, 20, 90), float32]) -> Tensor[(100, 64, 20, 90), float32] */;\n",
            "  %27 = %26(%25) /* ty=Tensor[(100, 64, 20, 90), float32] */;\n",
            "  %28 = fn (%p010: Tensor[(100, 64, 20, 90), float32] /* ty=Tensor[(100, 64, 20, 90), float32] */, %p16: Tensor[(32, 64, 1, 1), float32] /* ty=Tensor[(32, 64, 1, 1), float32] */, %p26: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1) -> Tensor[(100, 32, 20, 90), float32] {\n",
            "    %20 = nn.conv2d(%p010, %p16, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "    %21 = nn.bias_add(%20, %p26) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "    nn.relu(%21) /* ty=Tensor[(100, 32, 20, 90), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 20, 90), float32], Tensor[(32, 64, 1, 1), float32], Tensor[(32), float32]) -> Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %29 = %28(%27, meta[relay.Constant][2] /* ty=Tensor[(32, 64, 1, 1), float32] */, meta[relay.Constant][3] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %30 = fn (%p09: Tensor[(100, 32, 20, 90), float32] /* ty=Tensor[(100, 32, 20, 90), float32] */, %p15: Tensor[(32, 32, 3, 1), float32] /* ty=Tensor[(32, 32, 3, 1), float32] */, %p25: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1) -> Tensor[(100, 32, 20, 90), float32] {\n",
            "    %18 = nn.conv2d(%p09, %p15, padding=[1, 0, 1, 0], channels=32, kernel_size=[3, 1]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "    %19 = nn.bias_add(%18, %p25) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "    nn.relu(%19) /* ty=Tensor[(100, 32, 20, 90), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 32, 20, 90), float32], Tensor[(32, 32, 3, 1), float32], Tensor[(32), float32]) -> Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %31 = %30(%29, meta[relay.Constant][4] /* ty=Tensor[(32, 32, 3, 1), float32] */, meta[relay.Constant][5] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %32 = fn (%p08: Tensor[(100, 32, 20, 90), float32] /* ty=Tensor[(100, 32, 20, 90), float32] */, %p14: Tensor[(32, 32, 1, 3), float32] /* ty=Tensor[(32, 32, 1, 3), float32] */, %p24: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1) -> Tensor[(100, 32, 20, 90), float32] {\n",
            "    %16 = nn.conv2d(%p08, %p14, padding=[0, 1, 0, 1], channels=32, kernel_size=[1, 3]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "    %17 = nn.bias_add(%16, %p24) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "    nn.relu(%17) /* ty=Tensor[(100, 32, 20, 90), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 32, 20, 90), float32], Tensor[(32, 32, 1, 3), float32], Tensor[(32), float32]) -> Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %33 = %32(%31, meta[relay.Constant][6] /* ty=Tensor[(32, 32, 1, 3), float32] */, meta[relay.Constant][7] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
            "  %34 = fn (%p07: Tensor[(100, 32, 20, 90), float32] /* ty=Tensor[(100, 32, 20, 90), float32] */, %p13: Tensor[(128, 32, 1, 1), float32] /* ty=Tensor[(128, 32, 1, 1), float32] */, %p23: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1) -> Tensor[(100, 128, 20, 90), float32] {\n",
            "    %14 = nn.conv2d(%p07, %p13, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
            "    %15 = nn.bias_add(%14, %p23) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
            "    nn.relu(%15) /* ty=Tensor[(100, 128, 20, 90), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 32, 20, 90), float32], Tensor[(128, 32, 1, 1), float32], Tensor[(128), float32]) -> Tensor[(100, 128, 20, 90), float32] */;\n",
            "  %35 = %34(%33, meta[relay.Constant][8] /* ty=Tensor[(128, 32, 1, 1), float32] */, meta[relay.Constant][9] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
            "  %36 = fn (%p06: Tensor[(100, 128, 20, 90), float32] /* ty=Tensor[(100, 128, 20, 90), float32] */, Primitive=1) -> Tensor[(100, 128, 4, 18), float32] {\n",
            "    nn.avg_pool2d(%p06, pool_size=[5, 5], strides=[5, 5], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 128, 4, 18), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 128, 20, 90), float32]) -> Tensor[(100, 128, 4, 18), float32] */;\n",
            "  %38 = %36(%35) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
            "  %39 = fn (%p014: Tensor[(100, 128, 4, 18), float32] /* ty=Tensor[(100, 128, 4, 18), float32] */, Primitive=1) -> float32 {\n",
            "    %37 = power(%p014, 2f /* ty=float32 */) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
            "    mean(%37, axis=[0, 1, 2, 3]) /* ty=float32 */\n",
            "  } /* ty=fn (Tensor[(100, 128, 4, 18), float32]) -> float32 */;\n",
            "  %56 = fn (%p026: Tensor[(100, 128, 20, 90), float32] /* ty=Tensor[(100, 128, 20, 90), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 1, 128, 20, 90), float32] {\n",
            "    expand_dims(%p026, axis=1) /* ty=Tensor[(100, 1, 128, 20, 90), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 128, 20, 90), float32]) -> Tensor[(100, 1, 128, 20, 90), float32] */;\n",
            "  %57 = %56(%35) /* ty=Tensor[(100, 1, 128, 20, 90), float32] */;\n",
            "  %58 = fn (%p025: Tensor[(100, 1, 128, 20, 90), float32] /* ty=Tensor[(100, 1, 128, 20, 90), float32] */, Primitive=1) -> Tensor[(100, 1, 64, 18, 44), float32] {\n",
            "    nn.max_pool3d(%p025, pool_size=[1, 3, 3], strides=[2, 1, 2], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 1, 128, 20, 90), float32]) -> Tensor[(100, 1, 64, 18, 44), float32] */;\n",
            "  %59 = %58(%57) /* ty=Tensor[(100, 1, 64, 18, 44), float32] */;\n",
            "  %60 = fn (%p024: Tensor[(100, 1, 64, 18, 44), float32] /* ty=Tensor[(100, 1, 64, 18, 44), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
            "    squeeze(%p024, axis=[1]) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 1, 64, 18, 44), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %61 = %60(%59) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %62 = fn (%p023: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p114: Tensor[(64, 64, 1, 1), float32] /* ty=Tensor[(64, 64, 1, 1), float32] */, %p214: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
            "    %54 = nn.conv2d(%p023, %p114, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    %55 = nn.bias_add(%54, %p214) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    nn.relu(%55) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 1, 1), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %63 = %62(%61, meta[relay.Constant][10] /* ty=Tensor[(64, 64, 1, 1), float32] */, meta[relay.Constant][11] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %64 = fn (%p022: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p113: Tensor[(64, 64, 3, 1), float32] /* ty=Tensor[(64, 64, 3, 1), float32] */, %p213: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
            "    %52 = nn.conv2d(%p022, %p113, padding=[1, 0, 1, 0], channels=64, kernel_size=[3, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    %53 = nn.bias_add(%52, %p213) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    nn.relu(%53) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 3, 1), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %65 = %64(%63, meta[relay.Constant][12] /* ty=Tensor[(64, 64, 3, 1), float32] */, meta[relay.Constant][13] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %66 = fn (%p021: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p112: Tensor[(64, 64, 1, 3), float32] /* ty=Tensor[(64, 64, 1, 3), float32] */, %p212: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
            "    %50 = nn.conv2d(%p021, %p112, padding=[0, 1, 0, 1], channels=64, kernel_size=[1, 3]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    %51 = nn.bias_add(%50, %p212) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    nn.relu(%51) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 1, 3), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %67 = %66(%65, meta[relay.Constant][14] /* ty=Tensor[(64, 64, 1, 3), float32] */, meta[relay.Constant][15] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %68 = fn (%p020: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p111: Tensor[(256, 64, 1, 1), float32] /* ty=Tensor[(256, 64, 1, 1), float32] */, %p211: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, Primitive=1) -> Tensor[(100, 256, 18, 44), float32] {\n",
            "    %48 = nn.conv2d(%p020, %p111, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "    %49 = nn.bias_add(%48, %p211) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "    nn.relu(%49) /* ty=Tensor[(100, 256, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(256, 64, 1, 1), float32], Tensor[(256), float32]) -> Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %69 = %68(%67, meta[relay.Constant][16] /* ty=Tensor[(256, 64, 1, 1), float32] */, meta[relay.Constant][17] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %70 = fn (%p019: Tensor[(100, 256, 18, 44), float32] /* ty=Tensor[(100, 256, 18, 44), float32] */, %p110: Tensor[(64, 256, 1, 1), float32] /* ty=Tensor[(64, 256, 1, 1), float32] */, %p210: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
            "    %46 = nn.conv2d(%p019, %p110, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    %47 = nn.bias_add(%46, %p210) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    nn.relu(%47) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 256, 18, 44), float32], Tensor[(64, 256, 1, 1), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %71 = %70(%69, meta[relay.Constant][18] /* ty=Tensor[(64, 256, 1, 1), float32] */, meta[relay.Constant][19] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %72 = fn (%p018: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p19: Tensor[(64, 64, 3, 1), float32] /* ty=Tensor[(64, 64, 3, 1), float32] */, %p29: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
            "    %44 = nn.conv2d(%p018, %p19, padding=[1, 0, 1, 0], channels=64, kernel_size=[3, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    %45 = nn.bias_add(%44, %p29) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    nn.relu(%45) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 3, 1), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %73 = %72(%71, meta[relay.Constant][20] /* ty=Tensor[(64, 64, 3, 1), float32] */, meta[relay.Constant][21] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %74 = fn (%p017: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p18: Tensor[(64, 64, 1, 3), float32] /* ty=Tensor[(64, 64, 1, 3), float32] */, %p28: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
            "    %42 = nn.conv2d(%p017, %p18, padding=[0, 1, 0, 1], channels=64, kernel_size=[1, 3]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    %43 = nn.bias_add(%42, %p28) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "    nn.relu(%43) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 1, 3), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %75 = %74(%73, meta[relay.Constant][22] /* ty=Tensor[(64, 64, 1, 3), float32] */, meta[relay.Constant][23] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
            "  %76 = fn (%p016: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p17: Tensor[(256, 64, 1, 1), float32] /* ty=Tensor[(256, 64, 1, 1), float32] */, %p27: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, Primitive=1) -> Tensor[(100, 256, 18, 44), float32] {\n",
            "    %40 = nn.conv2d(%p016, %p17, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "    %41 = nn.bias_add(%40, %p27) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "    nn.relu(%41) /* ty=Tensor[(100, 256, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(256, 64, 1, 1), float32], Tensor[(256), float32]) -> Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %77 = %76(%75, meta[relay.Constant][24] /* ty=Tensor[(256, 64, 1, 1), float32] */, meta[relay.Constant][25] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
            "  %78 = fn (%p015: Tensor[(100, 256, 18, 44), float32] /* ty=Tensor[(100, 256, 18, 44), float32] */, Primitive=1) -> Tensor[(100, 256, 4, 18), float32] {\n",
            "    nn.avg_pool2d(%p015, pool_size=[4, 10], strides=[4, 2], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 256, 4, 18), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 256, 18, 44), float32]) -> Tensor[(100, 256, 4, 18), float32] */;\n",
            "  %80 = %78(%77) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
            "  %81 = fn (%p027: Tensor[(100, 256, 4, 18), float32] /* ty=Tensor[(100, 256, 4, 18), float32] */, Primitive=1) -> float32 {\n",
            "    %79 = power(%p027, 2f /* ty=float32 */) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
            "    mean(%79, axis=[0, 1, 2, 3]) /* ty=float32 */\n",
            "  } /* ty=fn (Tensor[(100, 256, 4, 18), float32]) -> float32 */;\n",
            "  %86 = fn (%p032: Tensor[(100, 256, 18, 44), float32] /* ty=Tensor[(100, 256, 18, 44), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 1, 256, 18, 44), float32] {\n",
            "    expand_dims(%p032, axis=1) /* ty=Tensor[(100, 1, 256, 18, 44), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 256, 18, 44), float32]) -> Tensor[(100, 1, 256, 18, 44), float32] */;\n",
            "  %87 = %86(%77) /* ty=Tensor[(100, 1, 256, 18, 44), float32] */;\n",
            "  %88 = fn (%p031: Tensor[(100, 1, 256, 18, 44), float32] /* ty=Tensor[(100, 1, 256, 18, 44), float32] */, Primitive=1) -> Tensor[(100, 1, 64, 16, 21), float32] {\n",
            "    nn.max_pool3d(%p031, pool_size=[1, 3, 3], strides=[4, 1, 2], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 16, 21), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 1, 256, 18, 44), float32]) -> Tensor[(100, 1, 64, 16, 21), float32] */;\n",
            "  %89 = %88(%87) /* ty=Tensor[(100, 1, 64, 16, 21), float32] */;\n",
            "  %90 = fn (%p030: Tensor[(100, 1, 64, 16, 21), float32] /* ty=Tensor[(100, 1, 64, 16, 21), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 64, 16, 21), float32] {\n",
            "    squeeze(%p030, axis=[1]) /* ty=Tensor[(100, 64, 16, 21), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 1, 64, 16, 21), float32]) -> Tensor[(100, 64, 16, 21), float32] */;\n",
            "  %91 = %90(%89) /* ty=Tensor[(100, 64, 16, 21), float32] */;\n",
            "  %92 = fn (%p029: Tensor[(100, 64, 16, 21), float32] /* ty=Tensor[(100, 64, 16, 21), float32] */, %p116: Tensor[(256, 64, 1, 4), float32] /* ty=Tensor[(256, 64, 1, 4), float32] */, %p216: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, Primitive=1) -> Tensor[(100, 256, 16, 18), float32] {\n",
            "    %84 = nn.conv2d(%p029, %p116, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 4]) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
            "    %85 = nn.bias_add(%84, %p216) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
            "    nn.relu(%85) /* ty=Tensor[(100, 256, 16, 18), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 16, 21), float32], Tensor[(256, 64, 1, 4), float32], Tensor[(256), float32]) -> Tensor[(100, 256, 16, 18), float32] */;\n",
            "  %93 = %92(%91, meta[relay.Constant][26] /* ty=Tensor[(256, 64, 1, 4), float32] */, meta[relay.Constant][27] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
            "  %94 = fn (%p028: Tensor[(100, 256, 16, 18), float32] /* ty=Tensor[(100, 256, 16, 18), float32] */, %p115: Tensor[(68, 256, 13, 1), float32] /* ty=Tensor[(68, 256, 13, 1), float32] */, %p215: Tensor[(68), float32] /* ty=Tensor[(68), float32] */, Primitive=1) -> Tensor[(100, 68, 4, 18), float32] {\n",
            "    %82 = nn.conv2d(%p028, %p115, padding=[0, 0, 0, 0], channels=68, kernel_size=[13, 1]) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "    %83 = nn.bias_add(%82, %p215) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "    nn.relu(%83) /* ty=Tensor[(100, 68, 4, 18), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 256, 16, 18), float32], Tensor[(68, 256, 13, 1), float32], Tensor[(68), float32]) -> Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %96 = %94(%93, meta[relay.Constant][28] /* ty=Tensor[(68, 256, 13, 1), float32] */, meta[relay.Constant][29] /* ty=Tensor[(68), float32] */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %97 = fn (%p033: Tensor[(100, 68, 4, 18), float32] /* ty=Tensor[(100, 68, 4, 18), float32] */, Primitive=1) -> float32 {\n",
            "    %95 = power(%p033, 2f /* ty=float32 */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "    mean(%95, axis=[0, 1, 2, 3]) /* ty=float32 */\n",
            "  } /* ty=fn (Tensor[(100, 68, 4, 18), float32]) -> float32 */;\n",
            "  %98 = %13(%12) /* ty=float32 */;\n",
            "  %99 = %39(%38) /* ty=float32 */;\n",
            "  %100 = %81(%80) /* ty=float32 */;\n",
            "  %101 = %97(%96) /* ty=float32 */;\n",
            "  %102 = fn (%p02: Tensor[(100, 64, 4, 18), float32] /* ty=Tensor[(100, 64, 4, 18), float32] */, %p11: float32 /* ty=float32 */, %p21: Tensor[(100, 128, 4, 18), float32] /* ty=Tensor[(100, 128, 4, 18), float32] */, %p3: float32 /* ty=float32 */, %p4: Tensor[(100, 256, 4, 18), float32] /* ty=Tensor[(100, 256, 4, 18), float32] */, %p5: float32 /* ty=float32 */, %p6: Tensor[(100, 68, 4, 18), float32] /* ty=Tensor[(100, 68, 4, 18), float32] */, %p7: float32 /* ty=float32 */, Primitive=1) -> Tensor[(100, 516, 4, 18), float32] {\n",
            "    %1 = divide(%p02, %p11) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
            "    %2 = divide(%p21, %p3) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
            "    %3 = divide(%p4, %p5) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
            "    %4 = divide(%p6, %p7) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "    %5 = (%1, %2, %3, %4) /* ty=(Tensor[(100, 64, 4, 18), float32], Tensor[(100, 128, 4, 18), float32], Tensor[(100, 256, 4, 18), float32], Tensor[(100, 68, 4, 18), float32]) */;\n",
            "    concatenate(%5, axis=1) /* ty=Tensor[(100, 516, 4, 18), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 64, 4, 18), float32], float32, Tensor[(100, 128, 4, 18), float32], float32, Tensor[(100, 256, 4, 18), float32], float32, Tensor[(100, 68, 4, 18), float32], float32) -> Tensor[(100, 516, 4, 18), float32] */;\n",
            "  %103 = %102(%12, %98, %38, %99, %80, %100, %96, %101) /* ty=Tensor[(100, 516, 4, 18), float32] */;\n",
            "  %104 = fn (%p01: Tensor[(100, 516, 4, 18), float32] /* ty=Tensor[(100, 516, 4, 18), float32] */, %p1: Tensor[(68, 516, 1, 1), float32] /* ty=Tensor[(68, 516, 1, 1), float32] */, %p2: Tensor[(68), float32] /* ty=Tensor[(68), float32] */, Primitive=1) -> Tensor[(100, 68, 4, 18), float32] {\n",
            "    %0 = nn.conv2d(%p01, %p1, padding=[0, 0, 0, 0], channels=68, kernel_size=[1, 1]) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "    nn.bias_add(%0, %p2) /* ty=Tensor[(100, 68, 4, 18), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 516, 4, 18), float32], Tensor[(68, 516, 1, 1), float32], Tensor[(68), float32]) -> Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %105 = %104(%103, meta[relay.Constant][30] /* ty=Tensor[(68, 516, 1, 1), float32] */, meta[relay.Constant][31] /* ty=Tensor[(68), float32] */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
            "  %106 = fn (%p0: Tensor[(100, 68, 4, 18), float32] /* ty=Tensor[(100, 68, 4, 18), float32] */, Primitive=1) -> Tensor[(100, 68, 18), float32] {\n",
            "    mean(%p0, axis=[2]) /* ty=Tensor[(100, 68, 18), float32] */\n",
            "  } /* ty=fn (Tensor[(100, 68, 4, 18), float32]) -> Tensor[(100, 68, 18), float32] */;\n",
            "  %106(%105) /* ty=Tensor[(100, 68, 18), float32] */\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from tvm import autotvm\n",
        "\n",
        "# # Example: Define a task and tune it\n",
        "# task = autotvm.task.create(\"conv2d\", args=(...), target=\"llvm\")  # Adjust for your model\n",
        "# measure_option = autotvm.measure_option(\n",
        "#     builder=autotvm.LocalBuilder(),\n",
        "#     runner=autotvm.LocalRunner(number=10, repeat=1, timeout=10)\n",
        "# )\n",
        "# tuner = autotvm.tuner.XGBTuner(task)\n",
        "# tuner.tune(n_trial=1000, measure_option=measure_option, callbacks=[autotvm.callback.log_to_file(\"tuning.log\")])"
      ],
      "metadata": {
        "id": "KSftZw2ug2f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# module = graph_executor.GraphModule(compiled_lib[\"default\"](dev))"
      ],
      "metadata": {
        "id": "EB6H4lZfB1mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a runtime executor\n",
        "# device = tvm.cpu()  # Replace with `tvm.cpu()` for CPU\n",
        "# module = graph_executor.GraphModule(lib[\"default\"](device))\n",
        "\n",
        "# # Set input data\n",
        "# import numpy as np\n",
        "# input_data = np.random.rand(1, 3, 24, 94).astype(\"float32\")\n",
        "# module.set_input(\"input\", tvm.nd.array(input_data))\n",
        "\n",
        "# # Run the model\n",
        "# module.run()\n",
        "\n",
        "# # Get output\n",
        "# output = module.get_output(0).asnumpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVXqaAsK5CZD",
        "outputId": "885a708f-bdc0-496c-b886-7a5c2b789f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-45.843307 , -26.777699 , -29.277887 , ..., -37.8795   ,\n",
              "         -26.984959 , -39.790222 ],\n",
              "        [-56.843357 , -35.934715 , -44.665154 , ..., -51.765587 ,\n",
              "         -43.806007 , -48.38035  ],\n",
              "        [-60.37148  , -40.050667 , -41.848427 , ..., -56.75559  ,\n",
              "         -59.2636   , -60.169075 ],\n",
              "        ...,\n",
              "        [-40.721634 , -63.13036  , -35.909454 , ..., -42.47039  ,\n",
              "         -35.262417 , -42.471115 ],\n",
              "        [-51.038795 , -84.98663  , -50.540886 , ..., -44.51516  ,\n",
              "         -44.781967 , -44.84289  ],\n",
              "        [ 41.37358  , -52.763165 ,   0.4645604, ..., -15.351716 ,\n",
              "           6.4474   , -11.702883 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acknowledgements: I would like to thank fellow classmate Adam Scott for giving me helpful advice when I was stuck on this project."
      ],
      "metadata": {
        "id": "TRJEzhyGsHvE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Model to ONNX"
      ],
      "metadata": {
        "id": "IssgZK7ztU7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import onnxruntime as ort\n",
        "\n",
        "# def test_onnx(onnx_file_path):\n",
        "#     ort_session = ort.InferenceSession(onnx_file_path)\n",
        "#     print(\"Loaded ONNX model successfully!\")\n",
        "\n",
        "#     test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "#     test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "#     try:\n",
        "#         Greedy_Decode_Eval_ONNX(ort_session, test_dataset, args)\n",
        "#     finally:\n",
        "#         cv2.destroyAllWindows()\n",
        "\n",
        "# def Greedy_Decode_Eval_ONNX(ort_session, datasets, args):\n",
        "#     epoch_size = len(datasets) // args.test_batch_size\n",
        "#     batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
        "\n",
        "#     Tp = 0\n",
        "#     Tn_1 = 0\n",
        "#     Tn_2 = 0\n",
        "#     t1 = time.time()\n",
        "#     for i in range(epoch_size):\n",
        "#         # Load batch data\n",
        "#         images, labels, lengths = next(batch_iterator)\n",
        "#         start = 0\n",
        "#         targets = []\n",
        "#         for length in lengths:\n",
        "#             label = labels[start:start+length]\n",
        "#             targets.append(label)\n",
        "#             start += length\n",
        "#         targets = np.array([el.numpy() for el in targets])\n",
        "#         imgs = images.numpy().copy()\n",
        "\n",
        "#         # ONNX inference\n",
        "#         ort_inputs = {ort_session.get_inputs()[0].name: images.numpy()}\n",
        "#         prebs = ort_session.run(None, ort_inputs)[0]\n",
        "\n",
        "#         # Greedy decode\n",
        "#         preb_labels = list()\n",
        "#         for i in range(prebs.shape[0]):\n",
        "#             preb = prebs[i, :, :]\n",
        "#             preb_label = list()\n",
        "#             for j in range(preb.shape[1]):\n",
        "#                 preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "#             no_repeat_blank_label = list()\n",
        "#             pre_c = preb_label[0]\n",
        "#             if pre_c != len(CHARS) - 1:\n",
        "#                 no_repeat_blank_label.append(pre_c)\n",
        "#             for c in preb_label:\n",
        "#                 if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "#                     if c == len(CHARS) - 1:\n",
        "#                         pre_c = c\n",
        "#                     continue\n",
        "#                 no_repeat_blank_label.append(c)\n",
        "#                 pre_c = c\n",
        "#             preb_labels.append(no_repeat_blank_label)\n",
        "\n",
        "#         # Evaluate accuracy\n",
        "#         for i, label in enumerate(preb_labels):\n",
        "#             if args.show:\n",
        "#                 show(imgs[i], label, targets[i])\n",
        "#             if len(label) != len(targets[i]):\n",
        "#                 Tn_1 += 1\n",
        "#                 continue\n",
        "#             if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "#                 Tp += 1\n",
        "#             else:\n",
        "#                 Tn_2 += 1\n",
        "#     Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "#     print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp + Tn_1 + Tn_2)))\n",
        "#     t2 = time.time()\n",
        "#     print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n"
      ],
      "metadata": {
        "id": "jSatJBmKoNrU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}