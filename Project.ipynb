{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvocQv11sMNb"
   },
   "source": [
    "# Optimizing LPRNet With MLC and Model Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RWufBmwsXL-"
   },
   "source": [
    "First, let's test the baseline LPRNet model. We will be using CPU for all demonstrative purposes due to the lack of compute resources on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-Ew775tsSb0"
   },
   "source": [
    "Import Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OKtB00a4m81F"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('test_data'):\n",
    "    !unzip test_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FKcO3sLDsUGZ"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVO0F8XOnFcL",
    "outputId": "336f9654-432c-45b4-9112-364231a56e73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in /usr/local/lib/python3.10/dist-packages (1.17.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
      "Requirement already satisfied: onnxscript in /usr/local/lib/python3.10/dist-packages (0.1.0.dev20241208)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.26.4)\n",
      "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.17.0)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
      "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.4.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxscript) (24.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.16->onnxscript) (4.25.5)\n",
      "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (1.20.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (4.25.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "!pip install onnxscript\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLDbrnh7sVVe"
   },
   "source": [
    "Import Libraries Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "DYAe4epAndFC"
   },
   "outputs": [],
   "source": [
    "from imutils import paths\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import *\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "from types import SimpleNamespace\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-pntVg6sZ1w"
   },
   "source": [
    "Define Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fJq318a4pe4a"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8rQhDd0sbUG"
   },
   "source": [
    "Model Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oziFh7oGwFQL"
   },
   "source": [
    "Due to issues with importing the MaxPool3D layer to ONNX, I needed to manually squeeze and unsqueeze those layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fSggeWdgni64"
   },
   "outputs": [],
   "source": [
    "class small_basic_block(nn.Module):\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(small_basic_block, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out // 4, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(ch_out // 4, ch_out, kernel_size=1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class LPRNet(nn.Module):\n",
    "    def __init__(self, lpr_max_len, phase, class_num, dropout_rate):\n",
    "        super(LPRNet, self).__init__()\n",
    "        self.phase = phase\n",
    "        self.lpr_max_len = lpr_max_len\n",
    "        self.class_num = class_num\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1), # 0\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),  # 2\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
    "            small_basic_block(ch_in=64, ch_out=128),    # *** 4 ***\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),  # 6\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
    "            small_basic_block(ch_in=64, ch_out=256),   # 8\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),  # 10\n",
    "            small_basic_block(ch_in=256, ch_out=256),   # *** 11 ***\n",
    "            nn.BatchNorm2d(num_features=256),   # 12\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),  # 14\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),  # 16\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),  # 18\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1), # 20\n",
    "            nn.BatchNorm2d(num_features=class_num),\n",
    "            nn.ReLU(),  # *** 22 ***\n",
    "        )\n",
    "        self.container = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=448+self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
    "            # nn.BatchNorm2d(num_features=self.class_num),\n",
    "            # nn.ReLU(),\n",
    "            # nn.Conv2d(in_channels=self.class_num, out_channels=self.lpr_max_len+1, kernel_size=3, stride=2),\n",
    "            # nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        keep_features = list()\n",
    "        for i, layer in enumerate(self.backbone.children()):\n",
    "            # x = layer(x)\n",
    "            if i == 3 or i == 7 or i == 14:\n",
    "                x = x.unsqueeze(1)\n",
    "                x = layer(x)\n",
    "                x = x.squeeze(1)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "            if i in [2, 6, 13, 22]: # [2, 4, 8, 11, 22]\n",
    "                keep_features.append(x)\n",
    "\n",
    "        global_context = list()\n",
    "        for i, f in enumerate(keep_features):\n",
    "            if i in [0, 1]:\n",
    "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
    "            if i in [2]:\n",
    "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
    "            f_pow = torch.pow(f, 2)\n",
    "            f_mean = torch.mean(f_pow)\n",
    "            f = torch.div(f, f_mean)\n",
    "            global_context.append(f)\n",
    "\n",
    "        x = torch.cat(global_context, 1)\n",
    "        x = self.container(x)\n",
    "        logits = torch.mean(x, dim=2)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def build_lprnet(lpr_max_len=8, phase=False, class_num=66, dropout_rate=0.5):\n",
    "\n",
    "    Net = LPRNet(lpr_max_len, phase, class_num, dropout_rate)\n",
    "\n",
    "    if phase == \"train\":\n",
    "        return Net.train()\n",
    "    else:\n",
    "        return Net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsawcRJFssAp"
   },
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sWZatftfn07j"
   },
   "outputs": [],
   "source": [
    "CHARS = ['京', '沪', '津', '渝', '冀', '晋', '蒙', '辽', '吉', '黑',\n",
    "         '苏', '浙', '皖', '闽', '赣', '鲁', '豫', '鄂', '湘', '粤',\n",
    "         '桂', '琼', '川', '贵', '云', '藏', '陕', '甘', '青', '宁',\n",
    "         '新',\n",
    "         '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "         'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K',\n",
    "         'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
    "         'W', 'X', 'Y', 'Z', 'I', 'O', '-'\n",
    "         ]\n",
    "\n",
    "CHARS_DICT = {char:i for i, char in enumerate(CHARS)}\n",
    "\n",
    "class LPRDataLoader(Dataset):\n",
    "    def __init__(self, img_dir, imgSize, lpr_max_len, PreprocFun=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = []\n",
    "        for i in range(len(img_dir)):\n",
    "            self.img_paths += [el for el in paths.list_images(img_dir[i])]\n",
    "        random.shuffle(self.img_paths)\n",
    "        self.img_size = imgSize\n",
    "        self.lpr_max_len = lpr_max_len\n",
    "        if PreprocFun is not None:\n",
    "            self.PreprocFun = PreprocFun\n",
    "        else:\n",
    "            self.PreprocFun = self.transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.img_paths[index]\n",
    "        Image = cv2.imread(filename)\n",
    "        height, width, _ = Image.shape\n",
    "        if height != self.img_size[1] or width != self.img_size[0]:\n",
    "            Image = cv2.resize(Image, self.img_size)\n",
    "        Image = self.PreprocFun(Image)\n",
    "\n",
    "        basename = os.path.basename(filename)\n",
    "        imgname, suffix = os.path.splitext(basename)\n",
    "        imgname = imgname.split(\"-\")[0].split(\"_\")[0]\n",
    "        label = list()\n",
    "        for c in imgname:\n",
    "            # one_hot_base = np.zeros(len(CHARS))\n",
    "            # one_hot_base[CHARS_DICT[c]] = 1\n",
    "            label.append(CHARS_DICT[c])\n",
    "\n",
    "        if len(label) == 8:\n",
    "            if self.check(label) == False:\n",
    "                print(imgname)\n",
    "                assert 0, \"Error label ^~^!!!\"\n",
    "\n",
    "        return Image, label, len(label)\n",
    "\n",
    "    def transform(self, img):\n",
    "        img = img.astype('float32')\n",
    "        img -= 127.5\n",
    "        img *= 0.0078125\n",
    "        img = np.transpose(img, (2, 0, 1))\n",
    "\n",
    "        return img\n",
    "\n",
    "    def check(self, label):\n",
    "        if label[2] != CHARS_DICT['D'] and label[2] != CHARS_DICT['F'] \\\n",
    "                and label[-1] != CHARS_DICT['D'] and label[-1] != CHARS_DICT['F']:\n",
    "            print(\"Error label, Please check!\")\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for _, sample in enumerate(batch):\n",
    "        img, label, length = sample\n",
    "        imgs.append(torch.from_numpy(img))\n",
    "        labels.extend(label)\n",
    "        lengths.append(length)\n",
    "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
    "\n",
    "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fBiy-QsMn8VA"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "    lprnet = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
    "    device = torch.device(\"cpu\")\n",
    "    lprnet.to(device)\n",
    "    print(\"Successful to build network!\")\n",
    "\n",
    "    # load pretrained model\n",
    "    if args.pretrained_model:\n",
    "        lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
    "        print(\"load pretrained model successful!\")\n",
    "    else:\n",
    "        print(\"[Error] Can't found pretrained mode, please check!\")\n",
    "        return False\n",
    "\n",
    "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
    "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
    "    try:\n",
    "        Greedy_Decode_Eval(lprnet, test_dataset, args)\n",
    "    finally:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def Greedy_Decode_Eval(Net, datasets, args):\n",
    "    # TestNet = Net.eval()\n",
    "    epoch_size = len(datasets) // args.test_batch_size\n",
    "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
    "\n",
    "    Tp = 0\n",
    "    Tn_1 = 0\n",
    "    Tn_2 = 0\n",
    "    t1 = time.time()\n",
    "    for i in range(epoch_size):\n",
    "        # load train data\n",
    "        images, labels, lengths = next(batch_iterator)\n",
    "        start = 0\n",
    "        targets = []\n",
    "        for length in lengths:\n",
    "            label = labels[start:start+length]\n",
    "            targets.append(label)\n",
    "            start += length\n",
    "        targets = np.array([el.numpy() for el in targets])\n",
    "        imgs = images.numpy().copy()\n",
    "\n",
    "        if args.cuda:\n",
    "            images = Variable(images.cuda())\n",
    "        else:\n",
    "            images = Variable(images)\n",
    "\n",
    "        # forward\n",
    "        prebs = Net(images)\n",
    "        # greedy decode\n",
    "        prebs = prebs.cpu().detach().numpy()\n",
    "        preb_labels = list()\n",
    "        for i in range(prebs.shape[0]):\n",
    "            preb = prebs[i, :, :]\n",
    "            preb_label = list()\n",
    "            for j in range(preb.shape[1]):\n",
    "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
    "            no_repeat_blank_label = list()\n",
    "            pre_c = preb_label[0]\n",
    "            if pre_c != len(CHARS) - 1:\n",
    "                no_repeat_blank_label.append(pre_c)\n",
    "            for c in preb_label: # dropout repeate label and blank label\n",
    "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
    "                    if c == len(CHARS) - 1:\n",
    "                        pre_c = c\n",
    "                    continue\n",
    "                no_repeat_blank_label.append(c)\n",
    "                pre_c = c\n",
    "            preb_labels.append(no_repeat_blank_label)\n",
    "        for i, label in enumerate(preb_labels):\n",
    "            # show image and its predict label\n",
    "            if args.show:\n",
    "                show(imgs[i], label, targets[i])\n",
    "            if len(label) != len(targets[i]):\n",
    "                Tn_1 += 1\n",
    "                continue\n",
    "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
    "                Tp += 1\n",
    "            else:\n",
    "                Tn_2 += 1\n",
    "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
    "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
    "    t2 = time.time()\n",
    "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n",
    "    return Acc\n",
    "\n",
    "def show(img, label, target):\n",
    "    img = np.transpose(img, (1, 2, 0))\n",
    "    img *= 128.\n",
    "    img += 127.5\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    lb = \"\"\n",
    "    for i in label:\n",
    "        lb += CHARS[i]\n",
    "    tg = \"\"\n",
    "    for j in target.tolist():\n",
    "        tg += CHARS[int(j)]\n",
    "\n",
    "    flag = \"F\"\n",
    "    if lb == tg:\n",
    "        flag = \"T\"\n",
    "    img = cv2ImgAddText(img, lb, (0, 0))\n",
    "    cv2.imshow(\"test\", img)\n",
    "    print(\"target: \", tg, \" ### {} ### \".format(flag), \"predict: \", lb)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def cv2ImgAddText(img, text, pos, textColor=(255, 0, 0), textSize=12):\n",
    "    if (isinstance(img, np.ndarray)):  # detect opencv format or not\n",
    "        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    fontText = ImageFont.truetype(\"data/NotoSansCJK-Regular.ttc\", textSize, encoding=\"utf-8\")\n",
    "    draw.text(pos, text, textColor, font=fontText)\n",
    "\n",
    "    return cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdv7Hiz_tHR1"
   },
   "source": [
    "Define Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OaRpp7CopKpc"
   },
   "outputs": [],
   "source": [
    "args = SimpleNamespace(**{\n",
    "    \"img_size\": [94, 24],\n",
    "    \"test_img_dirs\": \"./test_data\",\n",
    "    \"dropout_rate\": 0,\n",
    "    \"lpr_max_len\": 8,\n",
    "    \"test_batch_size\": 100,\n",
    "    \"phase_train\": False,\n",
    "    \"num_workers\": 0,\n",
    "    \"cuda\": False, # test on CPU\n",
    "    \"show\": False,\n",
    "    \"pretrained_model\": \"./Final_LPRNet_model.pth\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcQ6WTUZiNjt"
   },
   "source": [
    "Let's calculate the PyTorch baseline model's accuracy and inference speed per image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCYIeM4dTWxW"
   },
   "source": [
    "**Results of Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOdFcfjfghby",
    "outputId": "f03d084e-c607-4f70-8890-2ef6390ce93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful to build network!\n",
      "load pretrained model successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-b429f7fc8cf4>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test Accuracy: 0.899 [899:59:42:1000]\n",
      "[Info] Test Speed: 0.20391867566108704s 1/1000]\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wnKtgZ58xEzg"
   },
   "source": [
    "Build LPR Net Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nLK3Mrinnp6B"
   },
   "outputs": [],
   "source": [
    "def get_model_size(model, filename):\n",
    "  dummy_input = torch.randn(1, 3, 24, 94)\n",
    "  torch.onnx.export(\n",
    "      lprnet,\n",
    "      dummy_input,\n",
    "      filename,\n",
    "      input_names=[\"input\"],\n",
    "      output_names=[\"output\"],\n",
    "      dynamic_axes={\n",
    "          \"input\": {0: \"batch_size\"},\n",
    "          \"output\": {0: \"batch_size\"},\n",
    "      },\n",
    "  )\n",
    "  size_in_bytes = os.path.getsize(filename)\n",
    "  size_in_mb = size_in_bytes / (1024 * 1024)\n",
    "  return size_in_mb\n",
    "\n",
    "def get_full_model_size_pytorch(model):\n",
    "    torch.save(model, \"full_model.pth\")\n",
    "    size = os.path.getsize(\"full_model.pth\")\n",
    "    os.remove('full_model.pth')\n",
    "    return size / 1e6  # Size in MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Y-IFkt3oR0p",
    "outputId": "71faabea-81fe-41f1-d642-8643157cd7e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-42df1b960982>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 1.7072572708129883 MB\n"
     ]
    }
   ],
   "source": [
    "lprnet = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
    "lprnet.load_state_dict(torch.load(args.pretrained_model, map_location=torch.device('cpu')))\n",
    "print(\"Model Size: \" + str(get_model_size(lprnet, \"lprnet.onnx\")) + \" MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWqwzlhFwnqs"
   },
   "source": [
    "As we can see, we were able to obtain a test accuracy of 89.9%, an inference speed of 203.9 ms, and a model size of 1.71 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rv3WMS4TUWbP",
    "outputId": "0daa56a3-2dca-4193-d401-09ccc2535078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 1.82749 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Size: \" + str(get_full_model_size_pytorch(lprnet)) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4UMUTzSUjqk"
   },
   "source": [
    "We will also keep track of the PyTorch saving method, as it will be a useful comparison. And with this method the baseline model size is around 1.83 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1bzhotMi2xp"
   },
   "source": [
    "**Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2SPxDudjHAm"
   },
   "source": [
    "Let's trying doing a filter-based pruning optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0dgs6Kd3njUA"
   },
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "  test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
    "  test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
    "  try:\n",
    "      Acc = Greedy_Decode_Eval(model, test_dataset, args)\n",
    "  finally:\n",
    "      cv2.destroyAllWindows()\n",
    "  return Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ND1H9NVjjmbH",
    "outputId": "13a7fcaf-b902-4ea2-c475-e57d314b934b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LPRNet Model is on device: cpu\n"
     ]
    }
   ],
   "source": [
    "lprnet_copy = copy.deepcopy(lprnet)\n",
    "print(f\"LPRNet Model is on device: {next(lprnet_copy.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "e4mP_WQYkL7d"
   },
   "outputs": [],
   "source": [
    "def recover_model(model):\n",
    "    state_dict = torch.load(args.pretrained_model, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "MZh1YXPMjEr1"
   },
   "outputs": [],
   "source": [
    "def filter_prune(tensor: nn.Conv2d, sparsity: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Filter-based pruning for convolutional layers\n",
    "    :param tensor: torch.(cuda.)Tensor, weight of conv layer\n",
    "    :param sparsity: float, pruning sparsity\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    :return:\n",
    "        torch.(cuda.)Tensor, mask with zeroed indices\n",
    "    \"\"\"\n",
    "    num_filters = tensor.weight.size(0)\n",
    "    num_prune = int(num_filters * sparsity)\n",
    "\n",
    "    # L1-norm for each filter\n",
    "    flattened_tensor = tensor.weight.view(num_filters, -1)\n",
    "    filter_norms = torch.norm(flattened_tensor, p=1, dim=1)\n",
    "\n",
    "    # Prune filters with smallest L1-norm\n",
    "    prune_indices = torch.topk(filter_norms, num_prune, largest=False).indices\n",
    "\n",
    "    # Mask to determine what filters to keep / not keep\n",
    "    mask = torch.ones_like(filter_norms)\n",
    "    mask[prune_indices] = 0\n",
    "\n",
    "    # Apply mask to tensor weights\n",
    "    mask = mask.view(-1, 1, 1, 1)\n",
    "    tensor.weight.data.mul_(mask)\n",
    "\n",
    "    return mask\n",
    "\n",
    "class FilterPruner:\n",
    "    def __init__(self, model, sparsity_dict):\n",
    "        self.masks = FilterPruner.prune(model, sparsity_dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.masks:\n",
    "                param *= self.masks[name]\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def prune(model, sparsity_dict):\n",
    "        masks = dict()\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                if isinstance(sparsity_dict, dict):\n",
    "                    masks[name] = filter_prune(module, sparsity_dict[name])\n",
    "                else:\n",
    "                    assert(0 <= sparsity_dict < 1)\n",
    "                    if sparsity_dict > 0:\n",
    "                        masks[name] = filter_prune(module, sparsity_dict)\n",
    "        return masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9hM6h5X0Uc8"
   },
   "source": [
    "I orignally tried pruning up to 30% of the weight, but unfortunately it reduces the accuracy by a significant amount. As we have no efficient way of retraining the pruned model to gain back the lost accuracy, I decided to only prune the amount of weights that I could optimize, which ended up being only between 1% to 1.65%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH8MI2TejLXP",
    "outputId": "119aefea-25de-44b2-dad2-4b2f7e409816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity factor:  0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-6087602ff8ac>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(args.pretrained_model, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test Accuracy: 0.9 [900:59:41:1000]\n",
      "[Info] Test Speed: 0.19229235982894896s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.0105\n",
      "[Info] Test Accuracy: 0.9 [900:61:39:1000]\n",
      "[Info] Test Speed: 0.1845392234325409s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.011\n",
      "[Info] Test Accuracy: 0.9 [900:59:41:1000]\n",
      "[Info] Test Speed: 0.1847680263519287s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.0115\n",
      "[Info] Test Accuracy: 0.897 [897:61:42:1000]\n",
      "[Info] Test Speed: 0.18892252087593078s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.012\n",
      "[Info] Test Accuracy: 0.903 [903:58:39:1000]\n",
      "[Info] Test Speed: 0.18379838919639588s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.0125\n",
      "[Info] Test Accuracy: 0.902 [902:59:39:1000]\n",
      "[Info] Test Speed: 0.18459905409812927s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.013\n",
      "[Info] Test Accuracy: 0.897 [897:61:42:1000]\n",
      "[Info] Test Speed: 0.19133758211135865s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.0135\n",
      "[Info] Test Accuracy: 0.898 [898:60:42:1000]\n",
      "[Info] Test Speed: 0.19377629113197326s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.014\n",
      "[Info] Test Accuracy: 0.901 [901:58:41:1000]\n",
      "[Info] Test Speed: 0.18690786862373351s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.0145\n",
      "[Info] Test Accuracy: 0.899 [899:62:39:1000]\n",
      "[Info] Test Speed: 0.1831764578819275s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n",
      "Sparsity factor:  0.015\n",
      "[Info] Test Accuracy: 0.341 [341:495:164:1000]\n",
      "[Info] Test Speed: 0.1920521686077118s 1/1000]\n",
      "Model Size: 1.7072572708129883 MB\n"
     ]
    }
   ],
   "source": [
    "model_accuracies_filter = []\n",
    "model_sizes_filter = []\n",
    "best_pruned_model = None\n",
    "\n",
    "for i in range(20, 34, 1): # (20, 34, 1)\n",
    "  sparsity = i / 2000\n",
    "  print(\"Sparsity factor: \", sparsity)\n",
    "  lprnet_copy = recover_model(lprnet_copy)\n",
    "  pruner = FilterPruner(lprnet_copy, sparsity)\n",
    "  pruner.apply(lprnet_copy)\n",
    "  sparse_model_accuracy = evaluate(lprnet_copy)\n",
    "  sparse_model_size = get_model_size(lprnet_copy, \"lprnet_model_prune_\" + str(sparsity) + \".onnx\")\n",
    "  model_accuracies_filter.append(sparse_model_accuracy)\n",
    "  model_sizes_filter.append(sparse_model_size)\n",
    "  print(\"Model Size: \" + str(sparse_model_size) + \" MB\")\n",
    "  # As soon as the pruned model performs worse than 85%, I break from the for loop.\n",
    "  if sparse_model_accuracy > 0.85:\n",
    "    best_pruned_model = copy.deepcopy(lprnet_copy)\n",
    "  else:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1PFu4zXSuvD"
   },
   "source": [
    "**Results After Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADHJ-ONG11F4"
   },
   "source": [
    "As we can see from the output above, the model size did not change significantly as pruning was taking place, which makes sense since I could only remove such a small amount of weights before the model performance was affected dramatically. I will choose the best model which can maximize the sparsity while not affecting the accuracy or speed as much. From the graph above, and the values outputted, I believe that a sparsity factor of 1.45% would be best as it generally keeps the size the same at 1.707 MB while also changing the accuracy from 90.1% to 89.9%, and inference time from 203.9 to 183.1. Although these changes are somewhat negligble, I was not able to do much better due to the lack of a provided dataset which would enable me to fine-tune the model after pruning the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BHlHamJPT4y8",
    "outputId": "d46e82c4-86a3-43f0-c215-db34c2d83120"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of full model saved using PyTorch: 1.82749 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of full model saved using PyTorch: \" + str(get_full_model_size_pytorch(best_pruned_model)) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIlXEn58T_Rj"
   },
   "source": [
    "Since the size did not change, I decided to save the model using PyTorch, which keeps the weights and architecture, and I found that the best pruned model saved with a size of 1.82 MB. Comparing this to the baseline, we can also see that the model size did not change, suggesting that little improvement was done with pruning without dramatically affecting the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3UBfiuSSCg0T",
    "outputId": "8179edc4-78a4-40f2-c186-e3224c07d0a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LPRNet(\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): small_basic_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): small_basic_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): small_basic_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): ReLU()\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0))\n",
       "        (3): ReLU()\n",
       "        (4): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "        (5): ReLU()\n",
       "        (6): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): ReLU()\n",
       "    (14): MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Dropout(p=0, inplace=False)\n",
       "    (16): Conv2d(64, 256, kernel_size=(1, 4), stride=(1, 1))\n",
       "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Dropout(p=0, inplace=False)\n",
       "    (20): Conv2d(256, 68, kernel_size=(13, 1), stride=(1, 1))\n",
       "    (21): BatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU()\n",
       "  )\n",
       "  (container): Sequential(\n",
       "    (0): Conv2d(516, 68, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pruned_model_copy = copy.deepcopy(best_pruned_model)\n",
    "best_pruned_model_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9sFhnpgRt-Kl"
   },
   "source": [
    "**Post-Training Quantization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPFRcOqACkqP"
   },
   "source": [
    "After pruning, the next optimization that I chose to do was quantization. Specifically, I chose to apply post-training static quantization (PTQ) as I did not have a dataset to train the model after quantization. Nonetheless, we will apply this methodology to our model to see how it performs compared to pruning and the baseline itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAskWZIvnH32",
    "outputId": "d7b14bfa-592e-4f43-bcce-729707ffb1e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test Accuracy: 0.898 [898:61:41:1000]\n",
      "[Info] Test Speed: 0.18529540205001832s 1/1000]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.898"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(best_pruned_model_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVkzDcZVG0AY"
   },
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QTGY0TT4IXXN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3syTP6yIYT9"
   },
   "source": [
    "Import Test Dataset and Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wcydOHvQe5k3"
   },
   "outputs": [],
   "source": [
    "test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
    "test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False, num_workers=args.num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ayY06KISIk9D"
   },
   "source": [
    "For our specific methodology, we will need to wrap the MaxPool3D layers to ensure that those layers are contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "kukRO38tXiOK"
   },
   "outputs": [],
   "source": [
    "class EnsureContiguous(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.contiguous()\n",
    "\n",
    "def insert_quant_stubs(module):\n",
    "    for name, submodule in module.named_children():\n",
    "        if isinstance(submodule, nn.MaxPool3d):\n",
    "            new_layer = nn.Sequential(\n",
    "                DeQuantStub(),\n",
    "                EnsureContiguous(),\n",
    "                submodule,\n",
    "                QuantStub()\n",
    "            )\n",
    "            setattr(module, name, new_layer)\n",
    "        elif isinstance(submodule, nn.BatchNorm2d): # Did not work\n",
    "            # Skip BatchNorm2d layers\n",
    "            continue\n",
    "        elif isinstance(submodule, nn.Sequential):\n",
    "            insert_quant_stubs(submodule)\n",
    "    return module\n",
    "\n",
    "class LPRNetWithQuant(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper class to add quantization support to the LPRNet model.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        self.model = insert_quant_stubs(base_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "\n",
    "        feature_maps = []\n",
    "        for idx, layer in enumerate(self.model.backbone.children()):\n",
    "            x = layer(x)\n",
    "            if idx in [2, 6, 13, 22]:\n",
    "                feature_maps.append(x)\n",
    "\n",
    "        context_features = []\n",
    "        for idx, fmap in enumerate(feature_maps):\n",
    "            if idx in [0, 1]:\n",
    "                fmap = nn.AvgPool2d(kernel_size=5, stride=5)(fmap)\n",
    "            elif idx == 2:\n",
    "                fmap = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(fmap)\n",
    "\n",
    "            fmap = self.dequant(fmap)\n",
    "            normalized_fmap = fmap / torch.mean(torch.pow(fmap, 2))\n",
    "            fmap = self.quant(normalized_fmap)\n",
    "            context_features.append(fmap)\n",
    "\n",
    "        x = torch.cat(context_features, dim=1)\n",
    "        x = self.model.container(x)\n",
    "        logits = torch.mean(x, dim=2)\n",
    "        return self.dequant(logits).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTk_x1-PKa0H",
    "outputId": "1f6f5b49-5fd2-4b1a-8e61-493c53d019a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating batch 1/10\n",
      "Calibrating batch 2/10\n",
      "Calibrating batch 3/10\n",
      "Calibrating batch 4/10\n",
      "Calibrating batch 5/10\n",
      "Calibrating batch 6/10\n",
      "Calibrating batch 7/10\n",
      "Calibrating batch 8/10\n",
      "Calibrating batch 9/10\n",
      "Calibrating batch 10/10\n",
      "Quantization complete!\n"
     ]
    }
   ],
   "source": [
    "def prepare_model_for_quantization(base_model):\n",
    "    \"\"\"\n",
    "    Prepares the model for post-training quantization.\n",
    "    \"\"\"\n",
    "    quant_ready_model = LPRNetWithQuant(base_model)\n",
    "    quant_ready_model.eval()\n",
    "    return quant_ready_model\n",
    "\n",
    "def perform_calibration(model, data_loader, max_batches=100):\n",
    "    device = next(model.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, _, _) in enumerate(data_loader):\n",
    "            if batch_idx >= max_batches:\n",
    "                break\n",
    "            print(f\"Calibrating batch {batch_idx + 1}/{max_batches}\")\n",
    "            images = images.to(device)\n",
    "            model(images)\n",
    "\n",
    "# Configure quantization\n",
    "torch.backends.quantized.engine = 'fbgemm' # x86 architecture\n",
    "best_pruned_model_copy = prepare_model_for_quantization(best_pruned_model_copy)\n",
    "best_pruned_model_copy.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "# Prepare and calibrate\n",
    "torch.quantization.prepare(best_pruned_model_copy, inplace=True)\n",
    "perform_calibration(best_pruned_model_copy, test_loader, max_batches=10)\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(best_pruned_model_copy, inplace=True)\n",
    "\n",
    "print(\"Quantization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1Z_cKyNS1Bd"
   },
   "source": [
    "**Results After Quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDE-hu76CBSB",
    "outputId": "22bc6965-8902-47a5-d67f-46d505acf8e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test Accuracy: 0.725 [725:153:122:1000]\n",
      "[Info] Test Speed: 0.033321731090545655s 1/1000]\n",
      "Size of pruned and quantized model: 1.7072572708129883 MB\n"
     ]
    }
   ],
   "source": [
    "pruned_quantized_model = copy.deepcopy(best_pruned_model_copy)\n",
    "evaluate(pruned_quantized_model)\n",
    "print(\"Size of pruned and quantized model: \" + str(get_model_size(pruned_quantized_model, \"lprnet_model_prune_0.0145_quantized.onnx\")) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtMDvWmWD6Zt"
   },
   "source": [
    "The test accuracy reduced fairly significantly from 90% to 71.4% with quantization, but the inference speed also improved dramatically from 188.4 ms to 28.8 ms. Unfortunately, the model size does not change when I save it to an ONNX format, so let's trying saving it using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1zJpApXDxo_",
    "outputId": "6e0974bb-3a99-452b-bf06-c0b46fdfeae2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of full model saved using PyTorch: 0.536304 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of full model saved using PyTorch: \" + str(get_full_model_size_pytorch(pruned_quantized_model)) + \" MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wYRKg2dTfdP"
   },
   "source": [
    "From saving using PyTorch, which captures the weights and architecture but in a different format, I got a model size of 0.54 MB, which is significantly smaller than the 1.83 MB from pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pvLFSyvqVieV",
    "outputId": "4d81b477-7b15-4f7d-9002-18d208dcd28c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LPRNetWithQuant(\n",
       "  (quant): Quantize(scale=tensor([0.3139]), zero_point=tensor([3]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       "  (model): LPRNet(\n",
       "    (backbone): Sequential(\n",
       "      (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.3160325884819031, zero_point=56)\n",
       "      (1): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Sequential(\n",
       "        (0): DeQuantize()\n",
       "        (1): EnsureContiguous()\n",
       "        (2): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Quantize(scale=tensor([0.1786]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "      )\n",
       "      (4): small_basic_block(\n",
       "        (block): Sequential(\n",
       "          (0): QuantizedConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.42272183299064636, zero_point=71)\n",
       "          (1): ReLU()\n",
       "          (2): QuantizedConv2d(32, 32, kernel_size=(3, 1), stride=(1, 1), scale=0.8551036715507507, zero_point=69, padding=(1, 0))\n",
       "          (3): ReLU()\n",
       "          (4): QuantizedConv2d(32, 32, kernel_size=(1, 3), stride=(1, 1), scale=2.730538845062256, zero_point=42, padding=(0, 1))\n",
       "          (5): ReLU()\n",
       "          (6): QuantizedConv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), scale=3.6506521701812744, zero_point=65)\n",
       "        )\n",
       "      )\n",
       "      (5): QuantizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU()\n",
       "      (7): Sequential(\n",
       "        (0): DeQuantize()\n",
       "        (1): EnsureContiguous()\n",
       "        (2): MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Quantize(scale=tensor([0.4650]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "      )\n",
       "      (8): small_basic_block(\n",
       "        (block): Sequential(\n",
       "          (0): QuantizedConv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.8494337201118469, zero_point=60)\n",
       "          (1): ReLU()\n",
       "          (2): QuantizedConv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), scale=1.4190027713775635, zero_point=50, padding=(1, 0))\n",
       "          (3): ReLU()\n",
       "          (4): QuantizedConv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), scale=2.2697460651397705, zero_point=58, padding=(0, 1))\n",
       "          (5): ReLU()\n",
       "          (6): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=2.0976922512054443, zero_point=63)\n",
       "        )\n",
       "      )\n",
       "      (9): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (10): ReLU()\n",
       "      (11): small_basic_block(\n",
       "        (block): Sequential(\n",
       "          (0): QuantizedConv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.25665053725242615, zero_point=41)\n",
       "          (1): ReLU()\n",
       "          (2): QuantizedConv2d(64, 64, kernel_size=(3, 1), stride=(1, 1), scale=0.35725846886634827, zero_point=68, padding=(1, 0))\n",
       "          (3): ReLU()\n",
       "          (4): QuantizedConv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), scale=0.4292910397052765, zero_point=53, padding=(0, 1))\n",
       "          (5): ReLU()\n",
       "          (6): QuantizedConv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), scale=0.9712840914726257, zero_point=33)\n",
       "        )\n",
       "      )\n",
       "      (12): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (13): ReLU()\n",
       "      (14): Sequential(\n",
       "        (0): DeQuantize()\n",
       "        (1): EnsureContiguous()\n",
       "        (2): MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "        (3): Quantize(scale=tensor([0.0494]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "      )\n",
       "      (15): QuantizedDropout(p=0, inplace=False)\n",
       "      (16): QuantizedConv2d(64, 256, kernel_size=(1, 4), stride=(1, 1), scale=0.14894923567771912, zero_point=75)\n",
       "      (17): QuantizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (18): ReLU()\n",
       "      (19): QuantizedDropout(p=0, inplace=False)\n",
       "      (20): QuantizedConv2d(256, 68, kernel_size=(13, 1), stride=(1, 1), scale=0.21436432003974915, zero_point=68)\n",
       "      (21): QuantizedBatchNorm2d(68, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (22): ReLU()\n",
       "    )\n",
       "    (container): Sequential(\n",
       "      (0): QuantizedConv2d(516, 68, kernel_size=(1, 1), stride=(1, 1), scale=1.7822608947753906, zero_point=92)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_quantized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRmRljJayXjj"
   },
   "source": [
    "**TVM Optimizations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I could not load the pruned and quantized model into TVM due to incompatiblity issues with the QuantizedBatchNorm2D layers. Hence, I decided to optimize the baseline model using TVM instead to see how well MLC optimizations can perform. As we importing the model from ONNX, and TVM does not change the size of the model, the size will remain the same at 1.707 MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQJgOagI3aR5"
   },
   "source": [
    "Import Apache TVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qL8u14Z61enu",
    "outputId": "fcae8aa0-e9a4-4339-9b4d-812ce6df4974"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache-tvm in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (24.2.0)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (3.1.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.4.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.26.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (5.9.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.13.1)\n",
      "Requirement already satisfied: synr==0.6.0 in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (0.6.0)\n",
      "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (6.3.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install apache-tvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZF5eHqZ3ZmL",
    "outputId": "6b58653d-3da1-4b6f-b8e8-5e98a0d11663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%input: Tensor[(100, 3, 24, 94), float32] /* ty=Tensor[(100, 3, 24, 94), float32] */) -> Tensor[(100, 68, 18), float32] {\n",
      "  %0 = nn.conv2d(%input, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 3, 3), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
      "  %1 = nn.bias_add(%0, meta[relay.Constant][1] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
      "  %2 = nn.relu(%1) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
      "  %3 = nn.avg_pool2d(%2, pool_size=[5, 5], strides=[5, 5], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
      "  %4 = power(%3, 2f /* ty=float32 */) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
      "  %5 = mean(%4, axis=[0, 1, 2, 3]) /* ty=float32 */;\n",
      "  %6 = expand_dims(%2, axis=1) /* ty=Tensor[(100, 1, 64, 22, 92), float32] */;\n",
      "  %7 = nn.max_pool3d(%6, pool_size=[1, 3, 3], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 20, 90), float32] */;\n",
      "  %8 = squeeze(%7, axis=[1]) /* ty=Tensor[(100, 64, 20, 90), float32] */;\n",
      "  %9 = nn.conv2d(%8, meta[relay.Constant][2] /* ty=Tensor[(32, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %10 = nn.bias_add(%9, meta[relay.Constant][3] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %11 = nn.relu(%10) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %12 = nn.conv2d(%11, meta[relay.Constant][4] /* ty=Tensor[(32, 32, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=32, kernel_size=[3, 1]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %13 = nn.bias_add(%12, meta[relay.Constant][5] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %14 = nn.relu(%13) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %15 = nn.conv2d(%14, meta[relay.Constant][6] /* ty=Tensor[(32, 32, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=32, kernel_size=[1, 3]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %16 = nn.bias_add(%15, meta[relay.Constant][7] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %17 = nn.relu(%16) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %18 = nn.conv2d(%17, meta[relay.Constant][8] /* ty=Tensor[(128, 32, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
      "  %19 = nn.bias_add(%18, meta[relay.Constant][9] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
      "  %20 = nn.relu(%19) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
      "  %21 = nn.avg_pool2d(%20, pool_size=[5, 5], strides=[5, 5], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
      "  %22 = power(%21, 2f /* ty=float32 */) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
      "  %23 = mean(%22, axis=[0, 1, 2, 3]) /* ty=float32 */;\n",
      "  %24 = expand_dims(%20, axis=1) /* ty=Tensor[(100, 1, 128, 20, 90), float32] */;\n",
      "  %25 = nn.max_pool3d(%24, pool_size=[1, 3, 3], strides=[2, 1, 2], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 18, 44), float32] */;\n",
      "  %26 = squeeze(%25, axis=[1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %27 = nn.conv2d(%26, meta[relay.Constant][10] /* ty=Tensor[(64, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %28 = nn.bias_add(%27, meta[relay.Constant][11] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %29 = nn.relu(%28) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %30 = nn.conv2d(%29, meta[relay.Constant][12] /* ty=Tensor[(64, 64, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=64, kernel_size=[3, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %31 = nn.bias_add(%30, meta[relay.Constant][13] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %32 = nn.relu(%31) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %33 = nn.conv2d(%32, meta[relay.Constant][14] /* ty=Tensor[(64, 64, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=64, kernel_size=[1, 3]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %34 = nn.bias_add(%33, meta[relay.Constant][15] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %35 = nn.relu(%34) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %36 = nn.conv2d(%35, meta[relay.Constant][16] /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %37 = nn.bias_add(%36, meta[relay.Constant][17] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %38 = nn.relu(%37) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %39 = nn.conv2d(%38, meta[relay.Constant][18] /* ty=Tensor[(64, 256, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %40 = nn.bias_add(%39, meta[relay.Constant][19] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %41 = nn.relu(%40) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %42 = nn.conv2d(%41, meta[relay.Constant][20] /* ty=Tensor[(64, 64, 3, 1), float32] */, padding=[1, 0, 1, 0], channels=64, kernel_size=[3, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %43 = nn.bias_add(%42, meta[relay.Constant][21] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %44 = nn.relu(%43) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %45 = nn.conv2d(%44, meta[relay.Constant][22] /* ty=Tensor[(64, 64, 1, 3), float32] */, padding=[0, 1, 0, 1], channels=64, kernel_size=[1, 3]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %46 = nn.bias_add(%45, meta[relay.Constant][23] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %47 = nn.relu(%46) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %48 = nn.conv2d(%47, meta[relay.Constant][24] /* ty=Tensor[(256, 64, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %49 = nn.bias_add(%48, meta[relay.Constant][25] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %50 = nn.relu(%49) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %51 = nn.avg_pool2d(%50, pool_size=[4, 10], strides=[4, 2], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
      "  %52 = power(%51, 2f /* ty=float32 */) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
      "  %53 = mean(%52, axis=[0, 1, 2, 3]) /* ty=float32 */;\n",
      "  %54 = expand_dims(%50, axis=1) /* ty=Tensor[(100, 1, 256, 18, 44), float32] */;\n",
      "  %55 = nn.max_pool3d(%54, pool_size=[1, 3, 3], strides=[4, 1, 2], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 16, 21), float32] */;\n",
      "  %56 = squeeze(%55, axis=[1]) /* ty=Tensor[(100, 64, 16, 21), float32] */;\n",
      "  %57 = nn.conv2d(%56, meta[relay.Constant][26] /* ty=Tensor[(256, 64, 1, 4), float32] */, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 4]) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
      "  %58 = nn.bias_add(%57, meta[relay.Constant][27] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
      "  %59 = nn.relu(%58) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
      "  %60 = nn.conv2d(%59, meta[relay.Constant][28] /* ty=Tensor[(68, 256, 13, 1), float32] */, padding=[0, 0, 0, 0], channels=68, kernel_size=[13, 1]) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %61 = nn.bias_add(%60, meta[relay.Constant][29] /* ty=Tensor[(68), float32] */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %62 = nn.relu(%61) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %63 = power(%62, 2f /* ty=float32 */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %64 = mean(%63, axis=[0, 1, 2, 3]) /* ty=float32 */;\n",
      "  %65 = divide(%3, %5) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
      "  %66 = divide(%21, %23) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
      "  %67 = divide(%51, %53) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
      "  %68 = divide(%62, %64) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %69 = (%65, %66, %67, %68) /* ty=(Tensor[(100, 64, 4, 18), float32], Tensor[(100, 128, 4, 18), float32], Tensor[(100, 256, 4, 18), float32], Tensor[(100, 68, 4, 18), float32]) */;\n",
      "  %70 = concatenate(%69, axis=1) /* ty=Tensor[(100, 516, 4, 18), float32] */;\n",
      "  %71 = nn.conv2d(%70, meta[relay.Constant][30] /* ty=Tensor[(68, 516, 1, 1), float32] */, padding=[0, 0, 0, 0], channels=68, kernel_size=[1, 1]) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %72 = nn.bias_add(%71, meta[relay.Constant][31] /* ty=Tensor[(68), float32] */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  mean(%72, axis=[2]) /* ty=Tensor[(100, 68, 18), float32] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "import onnx\n",
    "from tvm.contrib import graph_executor\n",
    "\n",
    "onnx_model = onnx.load(\"lprnet_model_prune_0.0145_quantized.onnx\")\n",
    "\n",
    "mod, params = relay.frontend.from_onnx(onnx_model, shape={\"input\": (100, 3, 24, 94)})\n",
    "\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "M08bch-L0Z_k"
   },
   "outputs": [],
   "source": [
    "target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
    "dev = tvm.cpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wIHz48bfYLVd"
   },
   "outputs": [],
   "source": [
    "def build_module(mod):\n",
    "    with tvm.transform.PassContext(opt_level=3):\n",
    "        lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "    dtype = \"float32\"\n",
    "    module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "\n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "H_BPKr3JYL8n"
   },
   "outputs": [],
   "source": [
    "def Greedy_Decode_Eval_TVM(module, datasets, args):\n",
    "    epoch_size = len(datasets) // args.test_batch_size\n",
    "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
    "\n",
    "    Tp = 0\n",
    "    Tn_1 = 0\n",
    "    Tn_2 = 0\n",
    "    t1 = time.time()\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        # Load batch data\n",
    "        images, labels, lengths = next(batch_iterator)\n",
    "        start = 0\n",
    "        targets = []\n",
    "        for length in lengths:\n",
    "            label = labels[start:start + length]\n",
    "            targets.append(label)\n",
    "            start += length\n",
    "        targets = np.array([el.numpy() for el in targets])\n",
    "        imgs = images.numpy().copy()\n",
    "\n",
    "        # Set TVM inputs and run inference\n",
    "        module.set_input(\"input\", tvm.nd.array(images.numpy()))\n",
    "        module.run()\n",
    "        prebs = module.get_output(0).asnumpy()\n",
    "\n",
    "        # Greedy decode\n",
    "        preb_labels = list()\n",
    "        for i in range(prebs.shape[0]):\n",
    "            preb = prebs[i, :, :]\n",
    "            preb_label = list()\n",
    "            for j in range(preb.shape[1]):\n",
    "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
    "            no_repeat_blank_label = list()\n",
    "            pre_c = preb_label[0]\n",
    "            if pre_c != len(CHARS) - 1:\n",
    "                no_repeat_blank_label.append(pre_c)\n",
    "            for c in preb_label:\n",
    "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
    "                    if c == len(CHARS) - 1:\n",
    "                        pre_c = c\n",
    "                    continue\n",
    "                no_repeat_blank_label.append(c)\n",
    "                pre_c = c\n",
    "            preb_labels.append(no_repeat_blank_label)\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        for i, label in enumerate(preb_labels):\n",
    "            if args.show:\n",
    "                show(imgs[i], label, targets[i])\n",
    "            if len(label) != len(targets[i]):\n",
    "                Tn_1 += 1\n",
    "                continue\n",
    "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
    "                Tp += 1\n",
    "            else:\n",
    "                Tn_2 += 1\n",
    "\n",
    "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
    "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp + Tn_1 + Tn_2)))\n",
    "    t2 = time.time()\n",
    "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xD0I5SCfYbcu",
    "outputId": "f98f494e-9314-43c4-afc8-e3101de67749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test Accuracy: 0.899 [899:61:40:1000]\n",
      "[Info] Test Speed: 0.04791718316078186s 1/1000]\n"
     ]
    }
   ],
   "source": [
    "mod_base = build_module(mod)\n",
    "Greedy_Decode_Eval_TVM(mod_base, test_dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f6sYj-sbehE"
   },
   "source": [
    "After importing the ONNX model to TVM, the accuracy remains the same at around 89.9%. Moreover, the speed has also considerably reduced from the baseline, as it is now 47.9 ms from 203.9 ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmPARYGdd68e"
   },
   "source": [
    "Specific TVM Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZcICH9wjkfS"
   },
   "source": [
    "Computation Graph Optimization Pipeline (1st stage)\n",
    "\n",
    "* Graph Simplification: SimplifyInference, FoldConstant, FoldScaleAxis\n",
    "* Graph Pruning: DeadCodeElimination\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AtAab0_8Imi_"
   },
   "outputs": [],
   "source": [
    "mod_opt_2 = copy.deepcopy(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "-wqL2e-kgEK-"
   },
   "outputs": [],
   "source": [
    "from tvm.relay import transform\n",
    "\n",
    "# Define optimization passes\n",
    "passes = [\n",
    "    transform.SimplifyInference(),  # Simplify inference computations (e.g., BatchNorm folding)\n",
    "    transform.FoldConstant(),       # Fold constant computations\n",
    "    transform.FoldScaleAxis(),      # Fold scaling factors for performance gains\n",
    "    transform.DeadCodeElimination(),  # Remove unused outputs and operations\n",
    "]\n",
    "\n",
    "# Apply the optimization pipeline\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    mod_optimized_2 = tvm.transform.Sequential(passes)(mod_opt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNE6Ko42hZDX",
    "outputId": "358a9df7-b9ba-4d0a-8472-2fd815d0ad76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test Accuracy: 0.901 [901:59:40:1000]\n",
      "[Info] Test Speed: 0.04215845775604248s 1/1000]\n"
     ]
    }
   ],
   "source": [
    "mod_optimized_2_copy = copy.deepcopy(mod_optimized_2)\n",
    "mod_optimized_copy = build_module(mod_optimized_2_copy)\n",
    "Greedy_Decode_Eval_TVM(mod_optimized_copy, test_dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tp77GzZclsMG"
   },
   "source": [
    "As we can see, this optimization improved the accuracy, increasing it from 89.9% to 90.1%. The speed also decreased slightly, going from 47.9 ms to 42.1 ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOC4lX-ehvh9"
   },
   "source": [
    "Computation Graph Optimization Pipeline (2nd stage)\n",
    "* Computation Optimization: FuseOps, EliminateCommonSubexpr\n",
    "* Layout Optimization: AlterOpLayout, ConvertLayout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bL6QtqDSiOgP"
   },
   "outputs": [],
   "source": [
    "mod_opt_3 = copy.deepcopy(mod_opt_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "HZTkBhUyd5eu"
   },
   "outputs": [],
   "source": [
    "from tvm.relay import transform\n",
    "\n",
    "# Define optimization passes\n",
    "passes = [\n",
    "    transform.FuseOps(fuse_opt_level=3),  # Fuse Conv2D, ReLU, and BiasAdd into a single kernel\n",
    "    transform.AlterOpLayout(),  # Transform operations for better performance on the target hardware\n",
    "    transform.EliminateCommonSubexpr(),  # Remove duplicate computations\n",
    "    transform.ConvertLayout({\"conv2d\": [\"NCHW\", \"NHWC\"]}),\n",
    "]\n",
    "\n",
    "# Apply the optimization pipeline\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    mod_optimized_3 = tvm.transform.Sequential(passes)(mod_opt_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iitV8c_Y1Evl",
    "outputId": "a0502612-9a83-4bb6-cb91-fdef642a1eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Test Accuracy: 0.902 [902:59:39:1000]\n",
      "[Info] Test Speed: 0.041823950052261354s 1/1000]\n"
     ]
    }
   ],
   "source": [
    "mod_optimized_3_copy = copy.deepcopy(mod_optimized_3)\n",
    "mod_optimized_3_copy = build_module(mod_optimized_3_copy)\n",
    "Greedy_Decode_Eval_TVM(mod_optimized_3_copy, test_dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdKx3CYtmpb8"
   },
   "source": [
    "This optimization also improved the test speed slightly, decreasing it from 42.1 to 41.8 ms. Moreover, the accuracy increased slightly to 90.2% from 90.1%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyU8l-CTlPGT"
   },
   "source": [
    "**Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6Zms-NolP9w"
   },
   "source": [
    "In conclusion, we explored two separate ways to optimize the base LPRNet model. First, we applied model optimization including pruning and quantization. First, we applied Filter-Level Pruning, which slightly improved the inference time from 203.9 to 183.2 seconds but did not affect the size or accuracy. Next, we applied post-training static quantization, which improved the inference speed from 183.2 ms to just 27.5 and model size from 1.827 MB to 0.536 MB. The other strategy I tried was with MLC optimizations with TVM, where we applied graph-specific optimizations such as FoldConstant and SimplifyInference, as well as computation-specific optimizations with FuseOps and AlterOpLayout. These strategies helped reduce the inference time from 203.9 ms to 41.8 ms and improved the accuracy slightly from 89.9% to 90.2%. Unfortunately, I could not bring the quantized model to TVM due to incompatibility issues but the results seen here prove that it can be possible to utilize all of these optimization techniques sequentially to decrease model size and improve inference time without significantly impacting the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOSQ7bamncFN"
   },
   "source": [
    "Acknowledgements: I would like to thank fellow classmate Adam Scott for giving me helpful advice when I was stuck on this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0hdHA6TnWTq"
   },
   "source": [
    "**Dead Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All code after this is dead code, which marks attempts that I have made to further improve model performance such as using AutoTVM that did not end up working out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJhSPsFGk5y9"
   },
   "source": [
    "AutoTVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFuUohzDk6wh"
   },
   "source": [
    "I am not sure why, but AutoTVM crashes when I run it on my model. I have tried numerous methods to resolve the issue but nothing worked, so I decided to skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5h091FA3i5Le",
    "outputId": "07ad99af-009d-48fc-a301-cb26c3ded9bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  ID  |                       Task Description                        | Latency (ms) | Speed (GFLOPS) | Trials |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "|    0 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__10 |            - |              - |      0 |\n",
      "|    1 |                                    vm_mod_fused_nn_max_pool3d |            - |              - |      0 |\n",
      "|    2 |                                     vm_mod_fused_power_mean_3 |            - |              - |      0 |\n",
      "|    3 |                                  vm_mod_fused_nn_avg_pool2d_1 |            - |              - |      0 |\n",
      "|    4 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__11 |            - |              - |      0 |\n",
      "|    5 |                                  vm_mod_fused_nn_avg_pool2d_2 |            - |              - |      0 |\n",
      "|    6 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__7 |            - |              - |      0 |\n",
      "|    7 |                                  vm_mod_fused_nn_max_pool3d_2 |            - |              - |      0 |\n",
      "|    8 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__9 |            - |              - |      0 |\n",
      "|    9 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__8 |            - |              - |      0 |\n",
      "|   10 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__6 |            - |              - |      0 |\n",
      "|   11 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__2 |            - |              - |      0 |\n",
      "|   12 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__1 |            - |              - |      0 |\n",
      "|   13 |                                       vm_mod_fused_power_mean |            - |              - |      0 |\n",
      "|   14 |                                             vm_mod_fused_mean |            - |              - |      0 |\n",
      "|   15 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_78936bd10a2f2f7c_ |            - |              - |      0 |\n",
      "|   16 |                                     vm_mod_fused_power_mean_1 |            - |              - |      0 |\n",
      "|   17 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__5 |            - |              - |      0 |\n",
      "|   18 |                                  vm_mod_fused_nn_max_pool3d_1 |            - |              - |      0 |\n",
      "|   19 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__4 |            - |              - |      0 |\n",
      "|   20 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b_ |            - |              - |      0 |\n",
      "|   21 |                                     vm_mod_fused_power_mean_2 |            - |              - |      0 |\n",
      "|   22 | vm_mod_fused_layout_transform_layout_transform_nn_contrib_conv2d_NCHWc_expand_dims_expa_4e5b1d8889904a8b__3 |            - |              - |      0 |\n",
      "|   23 |                                    vm_mod_fused_nn_avg_pool2d |            - |              - |      0 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Estimated total latency: - ms\tTrials: 0\tUsed time : 4 s\tNext ID: 0\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tvm import auto_scheduler, relay, rpc\n",
    "from tvm.contrib import utils, ndk\n",
    "from tvm.relay import transform\n",
    "\n",
    "# Define the tuning task\n",
    "tasks, task_weights = auto_scheduler.extract_tasks(\n",
    "    mod_optimized_autotvm[\"main\"], params, target\n",
    ")\n",
    "\n",
    "# Define the log file to save the tuning records\n",
    "log_file = \"tuning_records.json\"\n",
    "\n",
    "# Set up the TaskScheduler\n",
    "tuner = auto_scheduler.TaskScheduler(tasks, task_weights)\n",
    "\n",
    "# Set up the runner and builder (adjust according to your environment)\n",
    "builder = auto_scheduler.LocalBuilder()\n",
    "runner = auto_scheduler.LocalRunner(repeat=10, min_repeat_ms=100, enable_cpu_cache_flush=True)\n",
    "\n",
    "# Run the tuning process\n",
    "tuner.tune(\n",
    "    auto_scheduler.TuningOptions(\n",
    "        num_measure_trials=24,  # Number of trials\n",
    "        builder=builder,\n",
    "        runner=runner,\n",
    "        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSHbvNBvb37L"
   },
   "outputs": [],
   "source": [
    "from tvm import auto_scheduler\n",
    "\n",
    "with auto_scheduler.ApplyHistoryBest(\"tuning_records.json\"):\n",
    "    with tvm.transform.PassContext(opt_level=3, config={\"relay.backend.use_auto_scheduler\": True}):\n",
    "        lib_optimized_autotvm = relay.build(mod_optimized_autotvm, target=target, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60DtV-C7cMYE"
   },
   "source": [
    "Evaluating TVM Optimized Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vn7yuv3ecLnI"
   },
   "outputs": [],
   "source": [
    "from tvm.contrib import graph_executor\n",
    "\n",
    "dtype = \"float32\"\n",
    "module_optimized_autotvm_graph = graph_executor.GraphModule(lib_optimized_autotvm[\"default\"](dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hUqnk8bscZzH"
   },
   "outputs": [],
   "source": [
    "Greedy_Decode_Eval_TVM(module_optimized_autotvm_graph, test_dataset, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ij72hx5wazgq"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MeGV6sP8Ioi",
    "outputId": "fd85be07-fc92-42c3-8545-09e615744b97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def @main(%input: Tensor[(100, 3, 24, 94), float32] /* ty=Tensor[(100, 3, 24, 94), float32] */) -> Tensor[(100, 68, 18), float32] {\n",
      "  %8 = fn (%p04: Tensor[(100, 3, 24, 94), float32] /* ty=Tensor[(100, 3, 24, 94), float32] */, %p12: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %p22: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 22, 92), float32] {\n",
      "    %6 = nn.conv2d(%p04, %p12, padding=[0, 0, 0, 0], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
      "    %7 = nn.bias_add(%6, %p22) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
      "    nn.relu(%7) /* ty=Tensor[(100, 64, 22, 92), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 3, 24, 94), float32], Tensor[(64, 3, 3, 3), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 22, 92), float32] */;\n",
      "  %9 = %8(%input, meta[relay.Constant][0] /* ty=Tensor[(64, 3, 3, 3), float32] */, meta[relay.Constant][1] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 22, 92), float32] */;\n",
      "  %10 = fn (%p03: Tensor[(100, 64, 22, 92), float32] /* ty=Tensor[(100, 64, 22, 92), float32] */, Primitive=1) -> Tensor[(100, 64, 4, 18), float32] {\n",
      "    nn.avg_pool2d(%p03, pool_size=[5, 5], strides=[5, 5], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 64, 4, 18), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 22, 92), float32]) -> Tensor[(100, 64, 4, 18), float32] */;\n",
      "  %12 = %10(%9) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
      "  %13 = fn (%p05: Tensor[(100, 64, 4, 18), float32] /* ty=Tensor[(100, 64, 4, 18), float32] */, Primitive=1) -> float32 {\n",
      "    %11 = power(%p05, 2f /* ty=float32 */) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
      "    mean(%11, axis=[0, 1, 2, 3]) /* ty=float32 */\n",
      "  } /* ty=fn (Tensor[(100, 64, 4, 18), float32]) -> float32 */;\n",
      "  %22 = fn (%p013: Tensor[(100, 64, 22, 92), float32] /* ty=Tensor[(100, 64, 22, 92), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 1, 64, 22, 92), float32] {\n",
      "    expand_dims(%p013, axis=1) /* ty=Tensor[(100, 1, 64, 22, 92), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 22, 92), float32]) -> Tensor[(100, 1, 64, 22, 92), float32] */;\n",
      "  %23 = %22(%9) /* ty=Tensor[(100, 1, 64, 22, 92), float32] */;\n",
      "  %24 = fn (%p012: Tensor[(100, 1, 64, 22, 92), float32] /* ty=Tensor[(100, 1, 64, 22, 92), float32] */, Primitive=1) -> Tensor[(100, 1, 64, 20, 90), float32] {\n",
      "    nn.max_pool3d(%p012, pool_size=[1, 3, 3], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 20, 90), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 1, 64, 22, 92), float32]) -> Tensor[(100, 1, 64, 20, 90), float32] */;\n",
      "  %25 = %24(%23) /* ty=Tensor[(100, 1, 64, 20, 90), float32] */;\n",
      "  %26 = fn (%p011: Tensor[(100, 1, 64, 20, 90), float32] /* ty=Tensor[(100, 1, 64, 20, 90), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 64, 20, 90), float32] {\n",
      "    squeeze(%p011, axis=[1]) /* ty=Tensor[(100, 64, 20, 90), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 1, 64, 20, 90), float32]) -> Tensor[(100, 64, 20, 90), float32] */;\n",
      "  %27 = %26(%25) /* ty=Tensor[(100, 64, 20, 90), float32] */;\n",
      "  %28 = fn (%p010: Tensor[(100, 64, 20, 90), float32] /* ty=Tensor[(100, 64, 20, 90), float32] */, %p16: Tensor[(32, 64, 1, 1), float32] /* ty=Tensor[(32, 64, 1, 1), float32] */, %p26: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1) -> Tensor[(100, 32, 20, 90), float32] {\n",
      "    %20 = nn.conv2d(%p010, %p16, padding=[0, 0, 0, 0], channels=32, kernel_size=[1, 1]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "    %21 = nn.bias_add(%20, %p26) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "    nn.relu(%21) /* ty=Tensor[(100, 32, 20, 90), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 20, 90), float32], Tensor[(32, 64, 1, 1), float32], Tensor[(32), float32]) -> Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %29 = %28(%27, meta[relay.Constant][2] /* ty=Tensor[(32, 64, 1, 1), float32] */, meta[relay.Constant][3] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %30 = fn (%p09: Tensor[(100, 32, 20, 90), float32] /* ty=Tensor[(100, 32, 20, 90), float32] */, %p15: Tensor[(32, 32, 3, 1), float32] /* ty=Tensor[(32, 32, 3, 1), float32] */, %p25: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1) -> Tensor[(100, 32, 20, 90), float32] {\n",
      "    %18 = nn.conv2d(%p09, %p15, padding=[1, 0, 1, 0], channels=32, kernel_size=[3, 1]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "    %19 = nn.bias_add(%18, %p25) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "    nn.relu(%19) /* ty=Tensor[(100, 32, 20, 90), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 32, 20, 90), float32], Tensor[(32, 32, 3, 1), float32], Tensor[(32), float32]) -> Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %31 = %30(%29, meta[relay.Constant][4] /* ty=Tensor[(32, 32, 3, 1), float32] */, meta[relay.Constant][5] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %32 = fn (%p08: Tensor[(100, 32, 20, 90), float32] /* ty=Tensor[(100, 32, 20, 90), float32] */, %p14: Tensor[(32, 32, 1, 3), float32] /* ty=Tensor[(32, 32, 1, 3), float32] */, %p24: Tensor[(32), float32] /* ty=Tensor[(32), float32] */, Primitive=1) -> Tensor[(100, 32, 20, 90), float32] {\n",
      "    %16 = nn.conv2d(%p08, %p14, padding=[0, 1, 0, 1], channels=32, kernel_size=[1, 3]) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "    %17 = nn.bias_add(%16, %p24) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "    nn.relu(%17) /* ty=Tensor[(100, 32, 20, 90), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 32, 20, 90), float32], Tensor[(32, 32, 1, 3), float32], Tensor[(32), float32]) -> Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %33 = %32(%31, meta[relay.Constant][6] /* ty=Tensor[(32, 32, 1, 3), float32] */, meta[relay.Constant][7] /* ty=Tensor[(32), float32] */) /* ty=Tensor[(100, 32, 20, 90), float32] */;\n",
      "  %34 = fn (%p07: Tensor[(100, 32, 20, 90), float32] /* ty=Tensor[(100, 32, 20, 90), float32] */, %p13: Tensor[(128, 32, 1, 1), float32] /* ty=Tensor[(128, 32, 1, 1), float32] */, %p23: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Primitive=1) -> Tensor[(100, 128, 20, 90), float32] {\n",
      "    %14 = nn.conv2d(%p07, %p13, padding=[0, 0, 0, 0], channels=128, kernel_size=[1, 1]) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
      "    %15 = nn.bias_add(%14, %p23) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
      "    nn.relu(%15) /* ty=Tensor[(100, 128, 20, 90), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 32, 20, 90), float32], Tensor[(128, 32, 1, 1), float32], Tensor[(128), float32]) -> Tensor[(100, 128, 20, 90), float32] */;\n",
      "  %35 = %34(%33, meta[relay.Constant][8] /* ty=Tensor[(128, 32, 1, 1), float32] */, meta[relay.Constant][9] /* ty=Tensor[(128), float32] */) /* ty=Tensor[(100, 128, 20, 90), float32] */;\n",
      "  %36 = fn (%p06: Tensor[(100, 128, 20, 90), float32] /* ty=Tensor[(100, 128, 20, 90), float32] */, Primitive=1) -> Tensor[(100, 128, 4, 18), float32] {\n",
      "    nn.avg_pool2d(%p06, pool_size=[5, 5], strides=[5, 5], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 128, 4, 18), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 128, 20, 90), float32]) -> Tensor[(100, 128, 4, 18), float32] */;\n",
      "  %38 = %36(%35) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
      "  %39 = fn (%p014: Tensor[(100, 128, 4, 18), float32] /* ty=Tensor[(100, 128, 4, 18), float32] */, Primitive=1) -> float32 {\n",
      "    %37 = power(%p014, 2f /* ty=float32 */) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
      "    mean(%37, axis=[0, 1, 2, 3]) /* ty=float32 */\n",
      "  } /* ty=fn (Tensor[(100, 128, 4, 18), float32]) -> float32 */;\n",
      "  %56 = fn (%p026: Tensor[(100, 128, 20, 90), float32] /* ty=Tensor[(100, 128, 20, 90), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 1, 128, 20, 90), float32] {\n",
      "    expand_dims(%p026, axis=1) /* ty=Tensor[(100, 1, 128, 20, 90), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 128, 20, 90), float32]) -> Tensor[(100, 1, 128, 20, 90), float32] */;\n",
      "  %57 = %56(%35) /* ty=Tensor[(100, 1, 128, 20, 90), float32] */;\n",
      "  %58 = fn (%p025: Tensor[(100, 1, 128, 20, 90), float32] /* ty=Tensor[(100, 1, 128, 20, 90), float32] */, Primitive=1) -> Tensor[(100, 1, 64, 18, 44), float32] {\n",
      "    nn.max_pool3d(%p025, pool_size=[1, 3, 3], strides=[2, 1, 2], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 1, 128, 20, 90), float32]) -> Tensor[(100, 1, 64, 18, 44), float32] */;\n",
      "  %59 = %58(%57) /* ty=Tensor[(100, 1, 64, 18, 44), float32] */;\n",
      "  %60 = fn (%p024: Tensor[(100, 1, 64, 18, 44), float32] /* ty=Tensor[(100, 1, 64, 18, 44), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
      "    squeeze(%p024, axis=[1]) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 1, 64, 18, 44), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %61 = %60(%59) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %62 = fn (%p023: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p114: Tensor[(64, 64, 1, 1), float32] /* ty=Tensor[(64, 64, 1, 1), float32] */, %p214: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
      "    %54 = nn.conv2d(%p023, %p114, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    %55 = nn.bias_add(%54, %p214) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    nn.relu(%55) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 1, 1), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %63 = %62(%61, meta[relay.Constant][10] /* ty=Tensor[(64, 64, 1, 1), float32] */, meta[relay.Constant][11] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %64 = fn (%p022: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p113: Tensor[(64, 64, 3, 1), float32] /* ty=Tensor[(64, 64, 3, 1), float32] */, %p213: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
      "    %52 = nn.conv2d(%p022, %p113, padding=[1, 0, 1, 0], channels=64, kernel_size=[3, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    %53 = nn.bias_add(%52, %p213) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    nn.relu(%53) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 3, 1), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %65 = %64(%63, meta[relay.Constant][12] /* ty=Tensor[(64, 64, 3, 1), float32] */, meta[relay.Constant][13] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %66 = fn (%p021: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p112: Tensor[(64, 64, 1, 3), float32] /* ty=Tensor[(64, 64, 1, 3), float32] */, %p212: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
      "    %50 = nn.conv2d(%p021, %p112, padding=[0, 1, 0, 1], channels=64, kernel_size=[1, 3]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    %51 = nn.bias_add(%50, %p212) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    nn.relu(%51) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 1, 3), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %67 = %66(%65, meta[relay.Constant][14] /* ty=Tensor[(64, 64, 1, 3), float32] */, meta[relay.Constant][15] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %68 = fn (%p020: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p111: Tensor[(256, 64, 1, 1), float32] /* ty=Tensor[(256, 64, 1, 1), float32] */, %p211: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, Primitive=1) -> Tensor[(100, 256, 18, 44), float32] {\n",
      "    %48 = nn.conv2d(%p020, %p111, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "    %49 = nn.bias_add(%48, %p211) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "    nn.relu(%49) /* ty=Tensor[(100, 256, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(256, 64, 1, 1), float32], Tensor[(256), float32]) -> Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %69 = %68(%67, meta[relay.Constant][16] /* ty=Tensor[(256, 64, 1, 1), float32] */, meta[relay.Constant][17] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %70 = fn (%p019: Tensor[(100, 256, 18, 44), float32] /* ty=Tensor[(100, 256, 18, 44), float32] */, %p110: Tensor[(64, 256, 1, 1), float32] /* ty=Tensor[(64, 256, 1, 1), float32] */, %p210: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
      "    %46 = nn.conv2d(%p019, %p110, padding=[0, 0, 0, 0], channels=64, kernel_size=[1, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    %47 = nn.bias_add(%46, %p210) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    nn.relu(%47) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 256, 18, 44), float32], Tensor[(64, 256, 1, 1), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %71 = %70(%69, meta[relay.Constant][18] /* ty=Tensor[(64, 256, 1, 1), float32] */, meta[relay.Constant][19] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %72 = fn (%p018: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p19: Tensor[(64, 64, 3, 1), float32] /* ty=Tensor[(64, 64, 3, 1), float32] */, %p29: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
      "    %44 = nn.conv2d(%p018, %p19, padding=[1, 0, 1, 0], channels=64, kernel_size=[3, 1]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    %45 = nn.bias_add(%44, %p29) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    nn.relu(%45) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 3, 1), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %73 = %72(%71, meta[relay.Constant][20] /* ty=Tensor[(64, 64, 3, 1), float32] */, meta[relay.Constant][21] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %74 = fn (%p017: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p18: Tensor[(64, 64, 1, 3), float32] /* ty=Tensor[(64, 64, 1, 3), float32] */, %p28: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Primitive=1) -> Tensor[(100, 64, 18, 44), float32] {\n",
      "    %42 = nn.conv2d(%p017, %p18, padding=[0, 1, 0, 1], channels=64, kernel_size=[1, 3]) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    %43 = nn.bias_add(%42, %p28) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "    nn.relu(%43) /* ty=Tensor[(100, 64, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(64, 64, 1, 3), float32], Tensor[(64), float32]) -> Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %75 = %74(%73, meta[relay.Constant][22] /* ty=Tensor[(64, 64, 1, 3), float32] */, meta[relay.Constant][23] /* ty=Tensor[(64), float32] */) /* ty=Tensor[(100, 64, 18, 44), float32] */;\n",
      "  %76 = fn (%p016: Tensor[(100, 64, 18, 44), float32] /* ty=Tensor[(100, 64, 18, 44), float32] */, %p17: Tensor[(256, 64, 1, 1), float32] /* ty=Tensor[(256, 64, 1, 1), float32] */, %p27: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, Primitive=1) -> Tensor[(100, 256, 18, 44), float32] {\n",
      "    %40 = nn.conv2d(%p016, %p17, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 1]) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "    %41 = nn.bias_add(%40, %p27) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "    nn.relu(%41) /* ty=Tensor[(100, 256, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 18, 44), float32], Tensor[(256, 64, 1, 1), float32], Tensor[(256), float32]) -> Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %77 = %76(%75, meta[relay.Constant][24] /* ty=Tensor[(256, 64, 1, 1), float32] */, meta[relay.Constant][25] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 18, 44), float32] */;\n",
      "  %78 = fn (%p015: Tensor[(100, 256, 18, 44), float32] /* ty=Tensor[(100, 256, 18, 44), float32] */, Primitive=1) -> Tensor[(100, 256, 4, 18), float32] {\n",
      "    nn.avg_pool2d(%p015, pool_size=[4, 10], strides=[4, 2], padding=[0, 0, 0, 0], count_include_pad=True) /* ty=Tensor[(100, 256, 4, 18), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 256, 18, 44), float32]) -> Tensor[(100, 256, 4, 18), float32] */;\n",
      "  %80 = %78(%77) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
      "  %81 = fn (%p027: Tensor[(100, 256, 4, 18), float32] /* ty=Tensor[(100, 256, 4, 18), float32] */, Primitive=1) -> float32 {\n",
      "    %79 = power(%p027, 2f /* ty=float32 */) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
      "    mean(%79, axis=[0, 1, 2, 3]) /* ty=float32 */\n",
      "  } /* ty=fn (Tensor[(100, 256, 4, 18), float32]) -> float32 */;\n",
      "  %86 = fn (%p032: Tensor[(100, 256, 18, 44), float32] /* ty=Tensor[(100, 256, 18, 44), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 1, 256, 18, 44), float32] {\n",
      "    expand_dims(%p032, axis=1) /* ty=Tensor[(100, 1, 256, 18, 44), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 256, 18, 44), float32]) -> Tensor[(100, 1, 256, 18, 44), float32] */;\n",
      "  %87 = %86(%77) /* ty=Tensor[(100, 1, 256, 18, 44), float32] */;\n",
      "  %88 = fn (%p031: Tensor[(100, 1, 256, 18, 44), float32] /* ty=Tensor[(100, 1, 256, 18, 44), float32] */, Primitive=1) -> Tensor[(100, 1, 64, 16, 21), float32] {\n",
      "    nn.max_pool3d(%p031, pool_size=[1, 3, 3], strides=[4, 1, 2], padding=[0, 0, 0, 0, 0, 0]) /* ty=Tensor[(100, 1, 64, 16, 21), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 1, 256, 18, 44), float32]) -> Tensor[(100, 1, 64, 16, 21), float32] */;\n",
      "  %89 = %88(%87) /* ty=Tensor[(100, 1, 64, 16, 21), float32] */;\n",
      "  %90 = fn (%p030: Tensor[(100, 1, 64, 16, 21), float32] /* ty=Tensor[(100, 1, 64, 16, 21), float32] */, Primitive=1, relay.reshape_only=1) -> Tensor[(100, 64, 16, 21), float32] {\n",
      "    squeeze(%p030, axis=[1]) /* ty=Tensor[(100, 64, 16, 21), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 1, 64, 16, 21), float32]) -> Tensor[(100, 64, 16, 21), float32] */;\n",
      "  %91 = %90(%89) /* ty=Tensor[(100, 64, 16, 21), float32] */;\n",
      "  %92 = fn (%p029: Tensor[(100, 64, 16, 21), float32] /* ty=Tensor[(100, 64, 16, 21), float32] */, %p116: Tensor[(256, 64, 1, 4), float32] /* ty=Tensor[(256, 64, 1, 4), float32] */, %p216: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, Primitive=1) -> Tensor[(100, 256, 16, 18), float32] {\n",
      "    %84 = nn.conv2d(%p029, %p116, padding=[0, 0, 0, 0], channels=256, kernel_size=[1, 4]) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
      "    %85 = nn.bias_add(%84, %p216) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
      "    nn.relu(%85) /* ty=Tensor[(100, 256, 16, 18), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 16, 21), float32], Tensor[(256, 64, 1, 4), float32], Tensor[(256), float32]) -> Tensor[(100, 256, 16, 18), float32] */;\n",
      "  %93 = %92(%91, meta[relay.Constant][26] /* ty=Tensor[(256, 64, 1, 4), float32] */, meta[relay.Constant][27] /* ty=Tensor[(256), float32] */) /* ty=Tensor[(100, 256, 16, 18), float32] */;\n",
      "  %94 = fn (%p028: Tensor[(100, 256, 16, 18), float32] /* ty=Tensor[(100, 256, 16, 18), float32] */, %p115: Tensor[(68, 256, 13, 1), float32] /* ty=Tensor[(68, 256, 13, 1), float32] */, %p215: Tensor[(68), float32] /* ty=Tensor[(68), float32] */, Primitive=1) -> Tensor[(100, 68, 4, 18), float32] {\n",
      "    %82 = nn.conv2d(%p028, %p115, padding=[0, 0, 0, 0], channels=68, kernel_size=[13, 1]) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "    %83 = nn.bias_add(%82, %p215) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "    nn.relu(%83) /* ty=Tensor[(100, 68, 4, 18), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 256, 16, 18), float32], Tensor[(68, 256, 13, 1), float32], Tensor[(68), float32]) -> Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %96 = %94(%93, meta[relay.Constant][28] /* ty=Tensor[(68, 256, 13, 1), float32] */, meta[relay.Constant][29] /* ty=Tensor[(68), float32] */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %97 = fn (%p033: Tensor[(100, 68, 4, 18), float32] /* ty=Tensor[(100, 68, 4, 18), float32] */, Primitive=1) -> float32 {\n",
      "    %95 = power(%p033, 2f /* ty=float32 */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "    mean(%95, axis=[0, 1, 2, 3]) /* ty=float32 */\n",
      "  } /* ty=fn (Tensor[(100, 68, 4, 18), float32]) -> float32 */;\n",
      "  %98 = %13(%12) /* ty=float32 */;\n",
      "  %99 = %39(%38) /* ty=float32 */;\n",
      "  %100 = %81(%80) /* ty=float32 */;\n",
      "  %101 = %97(%96) /* ty=float32 */;\n",
      "  %102 = fn (%p02: Tensor[(100, 64, 4, 18), float32] /* ty=Tensor[(100, 64, 4, 18), float32] */, %p11: float32 /* ty=float32 */, %p21: Tensor[(100, 128, 4, 18), float32] /* ty=Tensor[(100, 128, 4, 18), float32] */, %p3: float32 /* ty=float32 */, %p4: Tensor[(100, 256, 4, 18), float32] /* ty=Tensor[(100, 256, 4, 18), float32] */, %p5: float32 /* ty=float32 */, %p6: Tensor[(100, 68, 4, 18), float32] /* ty=Tensor[(100, 68, 4, 18), float32] */, %p7: float32 /* ty=float32 */, Primitive=1) -> Tensor[(100, 516, 4, 18), float32] {\n",
      "    %1 = divide(%p02, %p11) /* ty=Tensor[(100, 64, 4, 18), float32] */;\n",
      "    %2 = divide(%p21, %p3) /* ty=Tensor[(100, 128, 4, 18), float32] */;\n",
      "    %3 = divide(%p4, %p5) /* ty=Tensor[(100, 256, 4, 18), float32] */;\n",
      "    %4 = divide(%p6, %p7) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "    %5 = (%1, %2, %3, %4) /* ty=(Tensor[(100, 64, 4, 18), float32], Tensor[(100, 128, 4, 18), float32], Tensor[(100, 256, 4, 18), float32], Tensor[(100, 68, 4, 18), float32]) */;\n",
      "    concatenate(%5, axis=1) /* ty=Tensor[(100, 516, 4, 18), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 64, 4, 18), float32], float32, Tensor[(100, 128, 4, 18), float32], float32, Tensor[(100, 256, 4, 18), float32], float32, Tensor[(100, 68, 4, 18), float32], float32) -> Tensor[(100, 516, 4, 18), float32] */;\n",
      "  %103 = %102(%12, %98, %38, %99, %80, %100, %96, %101) /* ty=Tensor[(100, 516, 4, 18), float32] */;\n",
      "  %104 = fn (%p01: Tensor[(100, 516, 4, 18), float32] /* ty=Tensor[(100, 516, 4, 18), float32] */, %p1: Tensor[(68, 516, 1, 1), float32] /* ty=Tensor[(68, 516, 1, 1), float32] */, %p2: Tensor[(68), float32] /* ty=Tensor[(68), float32] */, Primitive=1) -> Tensor[(100, 68, 4, 18), float32] {\n",
      "    %0 = nn.conv2d(%p01, %p1, padding=[0, 0, 0, 0], channels=68, kernel_size=[1, 1]) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "    nn.bias_add(%0, %p2) /* ty=Tensor[(100, 68, 4, 18), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 516, 4, 18), float32], Tensor[(68, 516, 1, 1), float32], Tensor[(68), float32]) -> Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %105 = %104(%103, meta[relay.Constant][30] /* ty=Tensor[(68, 516, 1, 1), float32] */, meta[relay.Constant][31] /* ty=Tensor[(68), float32] */) /* ty=Tensor[(100, 68, 4, 18), float32] */;\n",
      "  %106 = fn (%p0: Tensor[(100, 68, 4, 18), float32] /* ty=Tensor[(100, 68, 4, 18), float32] */, Primitive=1) -> Tensor[(100, 68, 18), float32] {\n",
      "    mean(%p0, axis=[2]) /* ty=Tensor[(100, 68, 18), float32] */\n",
      "  } /* ty=fn (Tensor[(100, 68, 4, 18), float32]) -> Tensor[(100, 68, 18), float32] */;\n",
      "  %106(%105) /* ty=Tensor[(100, 68, 18), float32] */\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with tvm.transform.PassContext(opt_level=3):  # Level 3 is for heavy optimizations\n",
    "#     mod = relay.transform.FoldConstant()(mod)  # Fold constants into the graph (e.g., for batchnorm, conv, etc.)\n",
    "#     # mod = relay.transform.MergeComposite()(mod)  # This will try to merge compatible operators into one\n",
    "#     mod = relay.transform.FuseOps()(mod)  # Fuse operations like Conv2d + ReLU into a single kernel\n",
    "\n",
    "# # Step 4: Target the compilation (e.g., CPU or CUDA)\n",
    "# target = \"llvm\"  # Use \"cuda\" for GPU, \"llvm\" for CPU\n",
    "# dev = tvm.device(target, 0)\n",
    "\n",
    "# # Step 5: Compile the model with the applied optimizations\n",
    "# with tvm.transform.PassContext(opt_level=3):\n",
    "#     # Apply the optimizations and build the model for the target\n",
    "#     compiled_lib = relay.build(mod, target, params=params)\n",
    "\n",
    "# # Step 6: Check the compiled model (optional)\n",
    "# # You can print out the transformed Relay module or inspect the optimized operators\n",
    "# print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSftZw2ug2f7"
   },
   "outputs": [],
   "source": [
    "# from tvm import autotvm\n",
    "\n",
    "# # Example: Define a task and tune it\n",
    "# task = autotvm.task.create(\"conv2d\", args=(...), target=\"llvm\")  # Adjust for your model\n",
    "# measure_option = autotvm.measure_option(\n",
    "#     builder=autotvm.LocalBuilder(),\n",
    "#     runner=autotvm.LocalRunner(number=10, repeat=1, timeout=10)\n",
    "# )\n",
    "# tuner = autotvm.tuner.XGBTuner(task)\n",
    "# tuner.tune(n_trial=1000, measure_option=measure_option, callbacks=[autotvm.callback.log_to_file(\"tuning.log\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EB6H4lZfB1mt"
   },
   "outputs": [],
   "source": [
    "# module = graph_executor.GraphModule(compiled_lib[\"default\"](dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bVXqaAsK5CZD",
    "outputId": "885a708f-bdc0-496c-b886-7a5c2b789f56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-45.843307 , -26.777699 , -29.277887 , ..., -37.8795   ,\n",
       "         -26.984959 , -39.790222 ],\n",
       "        [-56.843357 , -35.934715 , -44.665154 , ..., -51.765587 ,\n",
       "         -43.806007 , -48.38035  ],\n",
       "        [-60.37148  , -40.050667 , -41.848427 , ..., -56.75559  ,\n",
       "         -59.2636   , -60.169075 ],\n",
       "        ...,\n",
       "        [-40.721634 , -63.13036  , -35.909454 , ..., -42.47039  ,\n",
       "         -35.262417 , -42.471115 ],\n",
       "        [-51.038795 , -84.98663  , -50.540886 , ..., -44.51516  ,\n",
       "         -44.781967 , -44.84289  ],\n",
       "        [ 41.37358  , -52.763165 ,   0.4645604, ..., -15.351716 ,\n",
       "           6.4474   , -11.702883 ]]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Create a runtime executor\n",
    "# device = tvm.cpu()  # Replace with `tvm.cpu()` for CPU\n",
    "# module = graph_executor.GraphModule(lib[\"default\"](device))\n",
    "\n",
    "# # Set input data\n",
    "# import numpy as np\n",
    "# input_data = np.random.rand(1, 3, 24, 94).astype(\"float32\")\n",
    "# module.set_input(\"input\", tvm.nd.array(input_data))\n",
    "\n",
    "# # Run the model\n",
    "# module.run()\n",
    "\n",
    "# # Get output\n",
    "# output = module.get_output(0).asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IssgZK7ztU7-"
   },
   "source": [
    "## Importing Model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jSatJBmKoNrU"
   },
   "outputs": [],
   "source": [
    "# import onnxruntime as ort\n",
    "\n",
    "# def test_onnx(onnx_file_path):\n",
    "#     ort_session = ort.InferenceSession(onnx_file_path)\n",
    "#     print(\"Loaded ONNX model successfully!\")\n",
    "\n",
    "#     test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
    "#     test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
    "#     try:\n",
    "#         Greedy_Decode_Eval_ONNX(ort_session, test_dataset, args)\n",
    "#     finally:\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "# def Greedy_Decode_Eval_ONNX(ort_session, datasets, args):\n",
    "#     epoch_size = len(datasets) // args.test_batch_size\n",
    "#     batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
    "\n",
    "#     Tp = 0\n",
    "#     Tn_1 = 0\n",
    "#     Tn_2 = 0\n",
    "#     t1 = time.time()\n",
    "#     for i in range(epoch_size):\n",
    "#         # Load batch data\n",
    "#         images, labels, lengths = next(batch_iterator)\n",
    "#         start = 0\n",
    "#         targets = []\n",
    "#         for length in lengths:\n",
    "#             label = labels[start:start+length]\n",
    "#             targets.append(label)\n",
    "#             start += length\n",
    "#         targets = np.array([el.numpy() for el in targets])\n",
    "#         imgs = images.numpy().copy()\n",
    "\n",
    "#         # ONNX inference\n",
    "#         ort_inputs = {ort_session.get_inputs()[0].name: images.numpy()}\n",
    "#         prebs = ort_session.run(None, ort_inputs)[0]\n",
    "\n",
    "#         # Greedy decode\n",
    "#         preb_labels = list()\n",
    "#         for i in range(prebs.shape[0]):\n",
    "#             preb = prebs[i, :, :]\n",
    "#             preb_label = list()\n",
    "#             for j in range(preb.shape[1]):\n",
    "#                 preb_label.append(np.argmax(preb[:, j], axis=0))\n",
    "#             no_repeat_blank_label = list()\n",
    "#             pre_c = preb_label[0]\n",
    "#             if pre_c != len(CHARS) - 1:\n",
    "#                 no_repeat_blank_label.append(pre_c)\n",
    "#             for c in preb_label:\n",
    "#                 if (pre_c == c) or (c == len(CHARS) - 1):\n",
    "#                     if c == len(CHARS) - 1:\n",
    "#                         pre_c = c\n",
    "#                     continue\n",
    "#                 no_repeat_blank_label.append(c)\n",
    "#                 pre_c = c\n",
    "#             preb_labels.append(no_repeat_blank_label)\n",
    "\n",
    "#         # Evaluate accuracy\n",
    "#         for i, label in enumerate(preb_labels):\n",
    "#             if args.show:\n",
    "#                 show(imgs[i], label, targets[i])\n",
    "#             if len(label) != len(targets[i]):\n",
    "#                 Tn_1 += 1\n",
    "#                 continue\n",
    "#             if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
    "#                 Tp += 1\n",
    "#             else:\n",
    "#                 Tn_2 += 1\n",
    "#     Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
    "#     print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp + Tn_1 + Tn_2)))\n",
    "#     t2 = time.time()\n",
    "#     print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
